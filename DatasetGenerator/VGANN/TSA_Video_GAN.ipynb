{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TSA_Video_GAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VHJaxZODbWW1","colab_type":"text"},"source":["# TSA Video GAN"]},{"cell_type":"code","metadata":{"id":"_z1Q35ioWYMi","colab_type":"code","outputId":"a93183d0-951e-4773-e263-1d196c8e9d56","executionInfo":{"status":"ok","timestamp":1575056311774,"user_tz":360,"elapsed":1193,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i-n0RJesWxOW","colab_type":"code","outputId":"ea9de5c2-febd-48cb-f6c6-57b663f10914","executionInfo":{"status":"ok","timestamp":1575056315125,"user_tz":360,"elapsed":4515,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%cd /content/drive/'My Drive'/PARMA/OpticalFlow/GitRepo/DatasetGenerator/VGANN\n","!ls"],"execution_count":21,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/DatasetGenerator/VGANN\n","TSA_Video_GAN.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VpCw9Ti0fDLl","colab_type":"text"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"NDa2NqKCinFD","colab_type":"text"},"source":["Select tensorflow 2.x"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2AVYrPMA38Bx","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Btuc_Y8XbqTb","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import tensorflow as tf\n","tf.__version__\n","\n","import glob\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","from tensorflow.keras import layers\n","import time\n","import cv2\n","\n","from IPython import display"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGZKgaOnfMrV","colab_type":"text"},"source":["## Model Definition"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Cu93_Y737rK"},"source":["### Creating the custom combination layer for the generator"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lkshFQBT365T","colab":{}},"source":["class MergeLayer(layers.Layer):\n","  def __init__(self):\n","    super(MergeLayer, self).__init__()\n"," \n","  @tf.function\n","  def call(self, fg, mask, bg):\n","\n","    # Fix bg shape \n","    fixed_bg = tf.stack([bg] * 32) \n","    fixed_bg = tf.transpose(fixed_bg, perm=[1, 2, 3, 0, 4])\n","    \n","    return tf.add( tf.multiply(mask, fg), tf.multiply(tf.add(-1.0, mask), fixed_bg) )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYdO_SmSfPz4","colab_type":"text"},"source":["### Generator"]},{"cell_type":"code","metadata":{"id":"vKXXP09nfOsV","colab_type":"code","colab":{}},"source":["def Two_Streams_Generator():\n","    \"\"\"\n","      This function creates a two stream generator architecture based on \n","      generating-videos-with-scene-dynamics paper\n","    \"\"\"\n","\n","\n","    # Recive the latent space\n","    latent_space = tf.keras.Input(shape=(100,))\n","\n","    # Foreground stream\n","    # Frist layer\n","    foreground = layers.Dense(4*4*2 * 512)(latent_space)\n","    foreground = layers.BatchNormalization()(foreground)\n","    foreground = layers.ReLU()(foreground)\n","    # Reshape and assert shape\n","    foreground = layers.Reshape((4, 4, 2, 512))(foreground)\n","    # Second layer \n","    foreground = layers.Conv3DTranspose(256, (2, 4, 4), 2, padding='same')(foreground)\n","    foreground = layers.BatchNormalization()(foreground)\n","    foreground = layers.ReLU()(foreground)\n","    # Third layer\n","    foreground = layers.Conv3DTranspose(128, (4, 4, 4), 2, padding='same')(foreground)\n","    foreground = layers.BatchNormalization()(foreground)\n","    foreground = layers.ReLU()(foreground)\n","    # Fourth layer\n","    foreground = layers.Conv3DTranspose(64, (4, 4, 4), 2, padding='same')(foreground)\n","    foreground = layers.BatchNormalization()(foreground)\n","    foreground = layers.ReLU()(foreground)\n","    # Fifth layer\n","    # Divide into mask and foreground\n","    mask = layers.Conv3DTranspose(1, (4, 4, 4), 2, padding='same')(foreground)\n","    foreground = layers.Conv3DTranspose(3, (4, 4, 4), 2, padding='same')(foreground)\n","    # Foreground\n","    foreground = layers.BatchNormalization()(foreground)\n","    foreground = tf.keras.activations.tanh(foreground)\n","    # Mask\n","    mask = layers.BatchNormalization()(mask)\n","    mask = tf.keras.activations.sigmoid(mask)\n","\n","\n","    # Background stream\n","    background = layers.Dense(4*4 * 512)(latent_space)\n","    background = layers.BatchNormalization()(background)\n","    background = layers.ReLU()(background)\n","    # Reshape and assert shape\n","    background = layers.Reshape((4, 4, 512))(background)\n","    # Second layer \n","    background = layers.Conv2DTranspose(256, (2, 4), 2, padding='same')(background)\n","    background = layers.BatchNormalization()(background)\n","    background = layers.ReLU()(background)\n","    # Third layer \n","    background = layers.Conv2DTranspose(128, (4, 4), 2, padding='same')(background)\n","    background = layers.BatchNormalization()(background)\n","    background = layers.ReLU()(background)\n","    # Fourth layer \n","    background = layers.Conv2DTranspose(64, (4, 4), 2, padding='same')(background)\n","    background = layers.BatchNormalization()(background)\n","    background = layers.ReLU()(background)\n","    # Fifth layer \n","    background = layers.Conv2DTranspose(3, (4, 4), 2, padding='same')(background)\n","    background = layers.BatchNormalization()(background)\n","    background = tf.keras.activations.tanh(background)\n","\n","    foreground = MergeLayer()(foreground, mask, background)\n","    # Dont compile the model since we want to update its weigths manually\n","    #model = tf.keras.Model(inputs=latent_space, outputs=[foreground, mask, background], name='TSA_Video_Generator')\n","    model = tf.keras.Model(inputs=latent_space, outputs=foreground, name='TSA_Video_Generator')\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GY1s7jVT47-T","colab_type":"text"},"source":["### Discriminator\n"]},{"cell_type":"code","metadata":{"id":"TYdqShDp5El3","colab_type":"code","colab":{}},"source":["def Video_Discrminator():\n","    \"\"\"\n","      This function creates a discriminator capable of recognizing real scenes and\n","      realisic frame motion\n","    \"\"\"\n","\n","    # Recive the video\n","    video = tf.keras.Input(shape=(64, 64, 32, 3,))\n","\n","    # Frist layer\n","    isReal = layers.Conv3D(64, (4, 4, 4), 2, padding='same')(video)\n","    isReal = layers.LeakyReLU()(isReal)\n","    # Second layer\n","    isReal = layers.Conv3D(128, (4, 4, 4), 2, padding='same')(isReal)\n","    isReal = layers.BatchNormalization()(isReal)\n","    isReal = layers.LeakyReLU()(isReal)\n","    # Third layer\n","    isReal = layers.Conv3D(256, (4, 4, 4), 2, padding='same')(isReal)\n","    isReal = layers.BatchNormalization()(isReal)\n","    isReal = layers.LeakyReLU()(isReal)\n","    # Fourth layer\n","    isReal = layers.Conv3D(512, (4, 4, 4), 2, padding='same')(isReal)\n","    isReal = layers.BatchNormalization()(isReal)\n","    isReal = layers.LeakyReLU()(isReal)\n","    # Fifth layer\n","    isReal = layers.Flatten()(isReal)\n","    isReal = layers.Dense(1, activation='sigmoid')(isReal)\n","    \n","    # Dont compile the model since we want to update its weigths manually\n","    model = tf.keras.Model(inputs=video, outputs=isReal, name='Video_Discrminator')\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6nEOsc_UWOj","colab_type":"text"},"source":["Create models"]},{"cell_type":"code","metadata":{"id":"HLtQP8JNUXnS","colab_type":"code","colab":{}},"source":["generator = Two_Streams_Generator()\n","discriminator = Video_Discrminator()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcBjY8PxPxPi","colab_type":"text"},"source":["Quick test to check everything is ok"]},{"cell_type":"code","metadata":{"id":"-LhPF3JTPwoR","colab_type":"code","outputId":"9b8a6e0d-5be7-4521-e217-5d746ac89e0d","executionInfo":{"status":"ok","timestamp":1575056321284,"user_tz":360,"elapsed":10526,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":303}},"source":["# Generate the seed\n","latent_space = tf.random.normal([1, 100])\n","generated_image = generator(latent_space, training=False)\n","# Plot false img \n","plt.imshow(generated_image[0, :, :, 0, :], cmap='gray')\n","# Print discriinator prediction\n","decision = discriminator(generated_image)\n","print (decision)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor([[0.50000656]], shape=(1, 1), dtype=float32)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBt2HAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Gsk7g8T4P3tV","colab_type":"text"},"source":["### Defining loss and optimizers\n","\n","In this case we will use cross entropy since its the recomended by the author"]},{"cell_type":"code","metadata":{"id":"JlvhTyfOAHKv","colab_type":"code","colab":{}},"source":["# This method returns a helper function to compute cross entropy loss\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZFO2XZyRhUb","colab_type":"text"},"source":["#### Discriminator loss\n","\n","This method quantifies how well the discriminator is able to distinguish real videos from fakes. It compares the discriminator's predictions on real videos to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."]},{"cell_type":"code","metadata":{"id":"8WHMTRdvRQ3z","colab_type":"code","colab":{}},"source":["def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aCHVedCiRv98","colab_type":"text"},"source":["### Generator loss\n","The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."]},{"cell_type":"code","metadata":{"id":"a7aLngHRRww3","colab_type":"code","colab":{}},"source":["def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJI6W_SFR-IM","colab_type":"text"},"source":["The discriminator and the generator optimizers are different since we will train two networks separately."]},{"cell_type":"code","metadata":{"id":"yzXUnZS4R_ID","colab_type":"code","colab":{}},"source":["generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2biWJr8VTk4","colab_type":"text"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"0AJcra6vU9QD","colab_type":"text"},"source":["Set checkpoints to important objects"]},{"cell_type":"code","metadata":{"id":"VERHWaA_U9ey","colab_type":"code","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n","                                 discriminator_optimizer=discriminator_optimizer,\n","                                 generator=generator,\n","                                 discriminator=discriminator)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vh2HbnD6VWA4","colab_type":"text"},"source":["### Set training parameters"]},{"cell_type":"code","metadata":{"id":"n_tW25A0VcBn","colab_type":"code","colab":{}},"source":["EPOCHS = 400\n","BATCH_SIZE = 32\n","\n","noise_dim = 100\n","num_examples_to_generate = 1\n","seed = tf.random.normal([num_examples_to_generate, noise_dim])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GkRMbATRVwEo","colab_type":"text"},"source":["The training loop begins with generator receiving a random seed as input. That seed is used to produce an video. The discriminator is then used to classify real videos (drawn from the training set) and fakes ones (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."]},{"cell_type":"code","metadata":{"id":"AKaUcdYrVw00","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(videos, batch_size):\n","    noise = tf.random.normal([batch_size, noise_dim])\n","\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        generated_videos = generator(noise, training=True)\n","\n","        real_output = discriminator(videos, training=True)\n","        fake_output = discriminator(generated_videos, training=True)\n","\n","        gen_loss = generator_loss(fake_output)\n","        disc_loss = discriminator_loss(real_output, fake_output)\n","\n","    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","\n","    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n","    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","\n","def get_batch(times, batch_size):\n","  a = (times - 1) * batch_size\n","  video_batch = tf.zeros([batch_size, 64, 64, 32, 3])\n","  for i in range(batch_size):\n","    a += i\n","    x = tf.image.resize(cv2.imread(\"/content/drive/'My Drive'/PARMA/OpticalFlow/GitRepo/Dataset/test/\" + str(a) + '.png'), [64, 64, 3])\n","    plt.imshow(x)\n","    input()\n","    #video_batch[i, 1, :, :, : ] = tf.image.resize(cv2.imread(\"/content/drive/'My Drive'/PARMA/OpticalFlow/GitRepo/Dataset/test/\" + str(a) + '.png'), [64, 64, 3])\n","  \n","  video_batch = tf.transpose(video_batch, perm=[1, 2, 3, 0, 4])\n","  return video_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aMWjXdoXBBT","colab_type":"code","colab":{}},"source":["def train(epochs, batch_size):\n","    for epoch in range(epochs):\n","        start = time.time()\n","\n","        for i in range(52):\n","            video_batch = get_batch(i, batch_size)\n","            train_step(video_batch, 1)\n","\n","        # Save the model every 15 epochs\n","        if (epoch + 1) % 15 == 0:\n","          checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n","\n","    print(50*'-')\n","    print('Training finished!')\n","    print(50*'-')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7INjI9PkX98C","colab_type":"code","outputId":"5d4843be-ff69-412f-bca0-ed5a7275e519","executionInfo":{"status":"error","timestamp":1575056321705,"user_tz":360,"elapsed":10832,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train(EPOCHS, BATCH_SIZE)"],"execution_count":38,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-b47aaf3f080c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-3fb0ef141c5c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mvideo_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-2c4bfa277710>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(times, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/'My Drive'/PARMA/OpticalFlow/GitRepo/Dataset/test/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images_v2\u001b[0;34m(images, size, method, preserve_aspect_ratio, antialias, name)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreserve_aspect_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m       skip_resize_if_same=False)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;34m\"\"\"Core functionality for v1 and v2 resize functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'images\\' contains no shape.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    284\u001b[0m                                          as_ref=False):\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."]}]},{"cell_type":"code","metadata":{"id":"FJfGVZTjRO0c","colab_type":"code","colab":{}},"source":["gen_vid = generator(seed)\n","\n","for i in range(32):\n","    plt.imshow(gen_vid[0, :, :, i, :] * 127.5 + 127.5)\n","    plt.savefig('image' + str(i) + '.png')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJlPRSbMYVWM","colab_type":"text"},"source":["Restore the latest checkpoint."]},{"cell_type":"code","metadata":{"id":"nWY2ueGCYShE","colab_type":"code","colab":{}},"source":["checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":0,"outputs":[]}]}