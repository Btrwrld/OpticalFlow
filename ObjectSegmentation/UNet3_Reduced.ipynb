{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UNet3_Reduced.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"g2-mVhXu4xJB","colab_type":"text"},"source":["# Install Dependencies\n","\n","The SciPy library is needed for the distance transform  to run.\n","\n"]},{"cell_type":"code","metadata":{"id":"YXWLX1mu5SV5","colab_type":"code","colab":{}},"source":["#!pip install scipy\n","#!pip install torchvision  \n","#!pip install --no-cache-dir -I pillow\n","\n","#!curl https://colab.chainer.org/install | sh -"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XZ68m-0p5kLc","colab_type":"text"},"source":["# Setup the Running Drive Folder\n","\n"]},{"cell_type":"code","metadata":{"id":"nk0yzwdS6YSo","colab_type":"code","outputId":"9d7d0587-33ec-44ee-db02-3984f152236f","executionInfo":{"status":"ok","timestamp":1566587208599,"user_tz":360,"elapsed":36140,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# Mount the drive folder. This will prompt for authorization.\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","# Opens the project folder. IMPORTANT: Change to your route\n","%cd 'drive/My Drive/PARMA/OpticalFlow/GitRepo/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive/PARMA/OpticalFlow/GitRepo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JNIr071X66-b","colab_type":"text"},"source":["# Modules Importation"]},{"cell_type":"code","metadata":{"id":"J6Y6my8a7Bf_","colab_type":"code","colab":{}},"source":["import sys\n","import os\n","import numpy as np\n","#import cupy as cp\n","import random\n","import time\n","import math\n","import csv\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch import argmax\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data.dataset import ConcatDataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","from torchvision.utils import make_grid\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms.functional as Ft\n","from torch.autograd.variable import Variable\n","\n","from scipy.ndimage.morphology import distance_transform_edt as dist_trans"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjHAPiB07FZp","colab_type":"text"},"source":["# Misc Functions"]},{"cell_type":"code","metadata":{"id":"4ZMKzkup7Lm_","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Export data to csv format. Creates new file if doesn't exist,\n","    otherwise update it.\n","    Args:\n","        header (list): headers of the column\n","        value (list): values of correspoding column\n","        folder: folder path\n","        file_name: file name with path\n","\"\"\"\n","def export_history(header, value, folder, file_name):\n","    # If folder does not exists make folder\n","    if not os.path.exists(folder):\n","        os.makedirs(folder)\n","\n","    file_path = folder + file_name\n","    file_existence = os.path.isfile(file_path)\n","\n","    # If there is no file make file\n","    if file_existence == False:\n","        file = open(file_path, 'w', newline='')\n","        writer = csv.writer(file)\n","        writer.writerow(header)\n","        writer.writerow(value)\n","    # If there is file overwrite\n","    else:\n","        file = open(file_path, 'a', newline='')\n","        writer = csv.writer(file)\n","        writer.writerow(value)\n","    # Close file when it is done with writing\n","    file.close()\n","\n","\n","\"\"\" \n","    Save the state of a net.\n","\"\"\"\n","def save_checkpoint(state, path='checkpoint/', filename='weights.pth'):\n","    # If folder does not exists make folder\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","    filepath = os.path.join(path, filename)\n","    torch.save(state, filepath)\n","\n","\n","\"\"\" \n","    Computes and stores the average and current value\n","    Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n","\"\"\"  \n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3MFjSGs77YTh","colab_type":"text"},"source":["# Definition of the Architecture\n","\n","As we show in the above image, U-Net consists on a big amounts of layers. In the following 2 cells all the parts of U-Net will be implemented. Each part have its own comment."]},{"cell_type":"code","metadata":{"id":"gS3XnHNX7kaw","colab_type":"code","colab":{}},"source":["\"\"\" \n","    This file defines every layer (or group of layers) that are inside UNet.\n","    At the final the architecture UNet is defined as a conjuntion of the elements created.\n","\"\"\"\n","class double_conv(nn.Module):\n","    ''' Applies (conv => BN => ReLU) two times. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(double_conv, self).__init__()\n","\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            # inplace is for aply ReLU to the original place, saving memory\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_ch),\n","            # inplace is for aply ReLU to the original place, saving memory\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class inconv(nn.Module):\n","    ''' First Section of U-Net. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(inconv, self).__init__()\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","\n","class down(nn.Module):\n","    ''' Applies a MaxPool with a Kernel of 2x2,\n","        then applies a double convolution pack. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(down, self).__init__()\n","\n","        self.mpconv = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2),\n","            double_conv(in_ch, out_ch)\n","        )\n","\n","    def forward(self, x):\n","        x = self.mpconv(x)\n","        return x\n","\n","\n","class up(nn.Module):\n","    ''' Applies a Deconvolution and then applies applies a double convolution pack. '''\n","\n","    def __init__(self, in_ch, out_ch, bilinear=False):\n","        super(up, self).__init__()\n","        \n","        # Bilinear is used to save computational cost\n","        if bilinear:\n","            self.up = nn.Upsample(\n","                scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            self.up = nn.ConvTranspose2d(\n","                in_ch//2, in_ch//2, kernel_size=2, stride=2)\n","\n","        self.conv = double_conv(in_ch, out_ch)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        diffX = x1.size()[2] - x2.size()[2]\n","        diffY = x1.size()[3] - x2.size()[3]\n","        x2 = F.pad(input=x2, pad=(diffX // 2, diffX // 2,\n","                                  diffY // 2, diffY // 2))\n","        \n","        x = torch.cat([x2, x1], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","\n","class outconv(nn.Module):\n","    ''' Applies the last Convolution to give an answer. '''\n","\n","    def __init__(self, in_ch, out_ch):\n","        super(outconv, self).__init__()\n","\n","        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return x\n","\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"INLK6iM87i70","colab_type":"code","colab":{}},"source":["class UNet(nn.Module):\n","    ''' This Object defines the architecture of U-Net. '''\n","\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        \n","        \"\"\"\n","        self.inc = inconv(n_channels, 64)\n","        self.down1 = down(64, 128)\n","        self.down2 = down(128, 256)\n","        self.down3 = down(256, 512)\n","        self.down4 = down(512, 512)\n","        self.up1 = up(1024, 256)\n","        self.up2 = up(512, 128)\n","        self.up3 = up(256, 64)\n","        self.up4 = up(128, 64)\n","        self.outc = outconv(64, n_classes)\n","        \"\"\"\n","        \n","        self.inc = inconv(n_channels, 16)\n","        self.down1 = down(16, 32)\n","        self.down2 = down(32, 64)\n","        self.down3 = down(64, 128)\n","        self.down4 = down(128, 128)\n","        self.up1 = up(256, 64)\n","        self.up2 = up(128, 32)\n","        self.up3 = up(64, 16)\n","        self.up4 = up(32, 16)\n","        self.outc = outconv(16, n_classes)\n","        \n","        \n","        \n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        x = self.outc(x)\n","        #x = F.softmax(x) # New softmax layer\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sNbQ4IW7yCm","colab_type":"text"},"source":["# Functions for Loading the Data\n","\n","The nexts cells are there for the loading process. In this case we use the Dataset and Dataloader objects given by Pytorch. In the training process we will call this section"]},{"cell_type":"code","metadata":{"id":"FCVc2fVig_Mk","colab_type":"code","colab":{}},"source":["'''\n","    Class that defines the reading and processing of the images.\n","    Specialized on BBBC006 dataset.\n","'''\n","class ImageDataset(Dataset):\n","    def __init__(self, ids, dir_data, dir_gt, extension='.png'):\n","\n","        self.dir_data = dir_data\n","        self.dir_gt = dir_gt\n","        self.extension = extension\n","\n","        # Transforms\n","        self.data_transforms = {\n","            'imgs': transforms.Compose([\n","#                 transforms.RandomResizedCrop(256),\n","#                 transforms.RandomHorizontalFlip(),\n","#                transforms.Resize((256, 256)),\n","                transforms.ToTensor(),\n","                #transforms.Normalize([0.0054],[0.0037])\n","            ]),\n","            'masks': transforms.Compose([\n","#                transforms.Resize((256, 256)),\n","                transforms.ToTensor()\n","            ]),\n","        }\n","\n","        # Images IDS\n","        self.ids = ids\n","\n","        # Calculate len of data\n","        self.data_len = len(self.ids)\n","\n","    '''\n","        Ask for an image.\n","    '''\n","    def __getitem__(self, index):\n","      \n","        # Get an ID of a specific image\n","        id_img = self.dir_data + self.ids[index] + self.extension\n","        id_gt = self.dir_gt + self.ids[index] + self.extension\n","        # Open Image and GroundTruth\n","        img = Image.open(id_img)#.convert('L')\n","        gt = Image.open(id_gt).convert('L')\n","        # Applies transformations\n","        img = self.data_transforms['imgs'](img)\n","        gt = self.data_transforms['masks'](gt)\n","        \n","        return (img, gt.float())\n","\n","    '''\n","        Length of the dataset.\n","        It's needed for the upper class.\n","    '''\n","    def __len__(self):\n","        return self.data_len\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpVYY1_T72aI","colab_type":"code","colab":{}},"source":["'''\n","    Class that defines the reading and processing of the images.\n","    Specialized on BBBC006 dataset.\n","'''\n","class ImageDataset_Transform(Dataset):\n","    def __init__(self, ids, dir_data, extension='.png'):\n","\n","        self.dir_data = dir_data\n","        self.extension = extension\n","\n","        # Transforms\n","        self.data_transforms = {\n","            'imgs': transforms.Compose([\n","#                 transforms.RandomResizedCrop(256),\n","#                 transforms.RandomHorizontalFlip(),\n","                transforms.Resize((256, 256)),\n","                transforms.ToTensor(),\n","                transforms.Normalize([0.0054],[0.0037])\n","            ]),\n","            'masks': transforms.Compose([\n","                transforms.Resize((256, 256)),\n","                transforms.ToTensor()\n","            ]),\n","        }\n","\n","        # Images IDS\n","        self.ids = ids\n","\n","        # Calculate len of data\n","        self.data_len = len(self.ids)\n","\n","    '''\n","        Ask for an image.\n","    '''\n","    def __getitem__(self, index):\n","        # Get an ID of a specific image\n","        id_img = self.dir_data + self.ids[index] + self.extension\n","        # Open Image and GroundTruth\n","        img = Image.open(id_img)#.convert('L')\n","        # Applies transformations\n","        img = self.data_transforms['imgs'](img)\n","        return (img, self.ids[index]+self.extension)\n","\n","    '''\n","        Length of the dataset.\n","        It's needed for the upper class.\n","    '''\n","    def __len__(self):\n","        return self.data_len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uhCw8gK76wf","colab_type":"code","colab":{}},"source":["'''\n","    Returns the dataset separated in batches.\n","    Used inside every epoch for retrieving the images.\n","'''\n","def get_dataloaders(dir_img, dir_gt, test_percent=0.2, batch_size=10):\n","  \n","    # Validate a correct percentage\n","    test_percent = test_percent/100 if test_percent > 1 else test_percent\n","    # Read the names of the images\n","    ids = [f[:-4] for f in os.listdir(dir_img)]\n","    # Creates the dataset\n","    dset = ImageDataset(ids, dir_img, dir_gt)\n","    \n","    # Calculate separation index for training and validation\n","    num_train = len(dset)\n","    indices = list(range(num_train))\n","    split = int(np.floor(test_percent * num_train))\n","    np.random.shuffle(indices)\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","\n","    # Create the dataloaders\n","    dataloaders = {}\n","    dataloaders['train'] = DataLoader(dset, batch_size=batch_size,\n","                                               sampler=SubsetRandomSampler(train_idx))\n","    dataloaders['val'] = DataLoader(dset, batch_size=batch_size,\n","                                                   sampler=SubsetRandomSampler(valid_idx))\n","   \n","    return dataloaders['train'], dataloaders['val']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4p0v6S2-hoFo","colab_type":"code","colab":{}},"source":["'''\n","    Returns whole dataset to transform.\n","'''\n","def get_dataloader_transform(dir_img, batch_size = 1):\n","    # Read the names of the images\n","    ids = [f[:-4] for f in os.listdir(dir_img)]\n","    # Creates the dataset\n","    dset = BBBCDataset_Transform(ids, dir_img)\n","\n","    # Create the dataloader\n","    dataloader = DataLoader(dset, batch_size=batch_size)\n","\n","    return dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4zOmy7-L8A84","colab_type":"text"},"source":["# Training the Model\n","\n","The next 3 cells are for training the model. Each part is commented."]},{"cell_type":"code","metadata":{"id":"b2oyt2y2OP8e","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Class that defines the Dice Loss function.\n","\"\"\"\n","class DiceLoss(nn.Module):\n","  \n","    def __init__(self, smooth = 1):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def dice_coef(self, y_pred, y_true):\n","        pred_probs = torch.sigmoid(y_pred)\n","        y_true_f = y_true.view(-1)\n","        y_pred_f = pred_probs.view(-1)\n","        intersection = torch.sum(y_true_f * y_pred_f)\n","        return (2. * intersection + self.smooth) / (torch.sum(y_true_f) + torch.sum(y_pred_f) + self.smooth)\n","  \n","    def forward(self, y_pred, y_true):\n","        return -self.dice_coef(y_pred, y_true)\n","    \n","\"\"\" \n","    Class that defines the Root Mean Square Loss function.\n","\"\"\"\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.mse = nn.MSELoss()\n","        \n","    def forward(self,yhat,y):\n","        return torch.sqrt(self.mse(yhat,y))\n","      \n","      \n","\"\"\"\n","    Class that defines the Cross Entropy Loss Function\n","\"\"\"\n","class CELoss(nn.Module):\n","    def __init__(self):\n","        super(CELoss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        return -torch.mean(torch.sum(y_true*torch.log(F.softmax(y_pred,dim=1)),dim=1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pB2zVEwN8IPa","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Functions that trains a net.\n","\"\"\"\n","def train_net(net, device, loader, optimizer, criterion, batch_size, loss_type):\n","    net.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    time_start = time.time()\n","    for batch_idx, (data, gt) in enumerate(loader):\n","\n","        # Use GPU or not\n","        data, gt = data.to(device), gt.to(device)\n","\n","        # Forward\n","        predictions = net(data)\n","        \n","        # Loss Calculation\n","        if loss_type == \"Cross\":\n","            labels = argmax(gt, dim=1)\n","            loss = criterion(predictions, labels)\n","        else:\n","            loss = criterion(predictions, gt)\n","            \n","        # Updates the record\n","        train_loss.update(loss.item(), predictions.size(0))\n","        train_acc.update(-loss.item(), predictions.size(0))\n","        \n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        # Does clipping of the gradient to avoid exploding gradient\n","        if loss_type == \"Grad\" or loss_type == \"Custom_Grad\":\n","          nn.utils.clip_grad_norm(net.parameters(), clip)\n","\n","        # Calculates new parameters\n","        optimizer.step()\n","\n","        print('[{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n","            batch_idx * len(data), len(loader)*batch_size,\n","            100. * batch_idx / len(loader), loss.item()))\n","\n","    time_dif = time.time() - time_start\n","    print('\\nAverage Training Loss: ' + str(train_loss.avg))\n","    print('\\nAverage Training Accuracy: ' + str(train_acc.avg))\n","    print('Train Time: It tooks %.4fs to finish the epoch.' % (time_dif))\n","            \n","    return train_loss.avg, train_acc.avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEuOs9AD8Mp-","colab_type":"code","colab":{}},"source":["\"\"\" \n","    Function that validates the net.\n","\"\"\"\n","def val_net(net, device, loader, criterion, batch_size):\n","    net.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    time_start = time.time()\n","    with torch.no_grad():\n","        for batch_idx, (data, gt) in enumerate(loader):\n","\n","            # Use GPU or not\n","            data, gt = data.to(device), gt.to(device)\n","\n","            # Forward\n","            predictions = net(data)\n","            \n","            # Loss Calculation\n","            loss = criterion(predictions, gt)\n","\n","            # Updates the record\n","            val_loss.update(loss.item(), predictions.size(0))\n","            val_acc.update(-loss.item(), predictions.size(0))\n","            \n","            print('[{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n","                batch_idx * len(data), len(loader)*batch_size,\n","                100. * batch_idx / len(loader), loss.item()))\n","    \n","    time_dif = time.time() - time_start\n","    print('\\nValidation set: Average loss: '+ str(val_loss.avg))\n","    print('\\nAverage Validation Accuracy: ' + str(val_acc.avg))\n","    print('Validation time: It tooks %.4fs to finish the Validation.' % (time_dif))\n","    \n","    return val_loss.avg, val_acc.avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0X6dcfJN8PV6","colab_type":"code","colab":{}},"source":["'''\n","    Configure every aspect of the run.\n","    Runs the training and validation.\n","'''\n","def setup_and_run_train(n_channels, n_classes, dir_img, dir_gt, dir_results, load, \n","                val_perc, batch_size, epochs, lr, run, optimizer, loss, loaders):\n","    \n","    # Use GPU or not\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    # Create the model\n","    net = UNet(n_channels, n_classes).to(device)\n","    net = torch.nn.DataParallel(net, device_ids=list(\n","        range(torch.cuda.device_count()))).to(device)\n","\n","    # Load old weights\n","    if load:\n","        net.load_state_dict(torch.load(load)['state_dict'])\n","        print('Model loaded from {}'.format(load))\n","\n","    if loaders:\n","      train_loader, val_loader = loaders[0], loaders[1]\n","      train_size = \"Fixed\"\n","      loader_size = \"Fixed\"\n","      \n","    else:\n","      \n","      train_loader, val_loader = get_dataloaders(dir_img, dir_gt, val_perc, batch_size)\n","            \n","      train_size = len(train_loader)*batch_size\n","      loader_size = len(val_loader)*batch_size\n","            \n","    # Pretty print of the run\n","    print('''\\n\n","    Starting training:\n","        Dataset: {}\n","        Num Channels: {}\n","        Groundtruth: {}\n","        Num Classes: {}\n","        Folder to save: {}\n","        Load previous: {}\n","        Training size: {}\n","        Validation size: {}\n","        Batch size: {}\n","        Epochs: {}\n","        Learning rate: {}\n","        Optimizer: {}\n","        Loss Function: {}\n","        CUDA: {}\n","    '''.format(dir_img, n_channels, dir_gt, n_classes, dir_results, load, \n","            train_size, loader_size, batch_size, epochs, lr, optimizer, loss, use_cuda))\n","\n","    # Definition of the optimizer ADD MORE IF YOU WANT\n","    if optimizer == \"Adam\":\n","        optimizer = torch.optim.Adam(net.parameters(),\n","                             lr=lr)\n","    elif optimizer == \"SGD\":\n","        optimizer = torch.optim.SGD(net.parameters(),\n","                        lr=lr,\n","                        momentum=0.9,\n","                        weight_decay=0.0005)\n","\n","    # Definition of the loss function ADD MORE IF YOU WANT\n","    if loss == \"Dice\":\n","        criterion = DiceLoss()\n","    elif loss == \"RMSE\":\n","        criterion = RMSELoss()\n","    elif loss == \"MSE\":\n","        criterion = nn.MSELoss()\n","    elif loss == \"MAE\":\n","        criterion = nn.L1Loss()\n","    elif loss == \"CE\":\n","        criterion = CELoss()\n","    elif loss == \"Cross\":\n","        weights = torch.tensor([0.5, 1.0, 2.0])\n","        weights = weights.to(device)\n","        \n","        criterion = nn.CrossEntropyLoss(weights)\n","    \n","\n","    # Saving History to csv\n","    header = ['epoch', 'train loss', 'validation loss']\n","\n","    best_loss = 10000\n","    time_start = time.time()\n","    # Run the training and validation\n","    for epoch in range(epochs):\n","        print('\\nStarting epoch {}/{}.'.format(epoch + 1, epochs))\n","\n","        train_loss, train_acc = train_net(net, device, train_loader, optimizer, criterion, batch_size, loss)\n","        val_loss, val_acc = val_net(net, device, val_loader, criterion, batch_size)\n","        \n","        values = [epoch+1, train_loss, val_loss]\n","        export_history(header, values, dir_results, \"result\"+str(run)+\".csv\")\n","        \n","        # save model\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            save_checkpoint({\n","                    'epoch': epoch + 1,\n","                    'state_dict': net.state_dict(),\n","                    'loss': train_loss,\n","                    'optimizer' : optimizer.state_dict(),\n","                }, path=dir_results, filename=\"weights\"+str(run)+\".pth\")\n","\n","    time_dif = time.time() - time_start\n","    print(\"It tooks %.4f seconds to finish the run.\" % (time_dif))\n","    \n","    return train_loss, val_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q95OsA9I8Tzp","colab_type":"text"},"source":["# Running the Training\n","\n","Now, we will run the training. You can change this hyperparameters to see how affects the results.\n","\n","**Only one run**"]},{"cell_type":"markdown","metadata":{"id":"yZEOfUV96YAd","colab_type":"text"},"source":["## Back Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6xJxnlcIxEpT","outputId":"a56d70f0-f04d-4788-ca31-43a567c00c38","executionInfo":{"status":"ok","timestamp":1566587850340,"user_tz":360,"elapsed":619083,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["setup_and_run_train(n_channels = 3, \n","                    n_classes = 1, \n","                    dir_img = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/sen/', \n","                    dir_gt = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/gt/', \n","                    dir_results = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/ObjectSegmentation/weights/', \n","                    #load = 'checkpoints/DT/weights1.pth', \n","                    load = False,\n","                    val_perc = 20, \n","                    batch_size = 10, \n","                    epochs = 10, \n","                    lr = 0.001, \n","                    run = str(1), \n","                    optimizer = \"Adam\", \n","                    loss = \"MSE\", \n","                    loaders = False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["\n","\n","    Starting training:\n","        Dataset: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/sen/\n","        Num Channels: 3\n","        Groundtruth: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/gt/\n","        Num Classes: 1\n","        Folder to save: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/ObjectSegmentation/weights/\n","        Load previous: False\n","        Training size: 800\n","        Validation size: 200\n","        Batch size: 10\n","        Epochs: 10\n","        Learning rate: 0.001\n","        Optimizer: Adam\n","        Loss Function: MSE\n","        CUDA: True\n","    \n","\n","Starting epoch 1/10.\n","[0/800 (0%)]\t\tLoss: 0.048209\n","[10/800 (1%)]\t\tLoss: 0.033924\n","[20/800 (2%)]\t\tLoss: 0.024958\n","[30/800 (4%)]\t\tLoss: 0.018715\n","[40/800 (5%)]\t\tLoss: 0.014656\n","[50/800 (6%)]\t\tLoss: 0.011517\n","[60/800 (8%)]\t\tLoss: 0.009066\n","[70/800 (9%)]\t\tLoss: 0.007272\n","[80/800 (10%)]\t\tLoss: 0.005967\n","[90/800 (11%)]\t\tLoss: 0.005000\n","[100/800 (12%)]\t\tLoss: 0.004242\n","[110/800 (14%)]\t\tLoss: 0.003694\n","[120/800 (15%)]\t\tLoss: 0.003171\n","[130/800 (16%)]\t\tLoss: 0.002801\n","[140/800 (18%)]\t\tLoss: 0.002438\n","[150/800 (19%)]\t\tLoss: 0.002198\n","[160/800 (20%)]\t\tLoss: 0.001910\n","[170/800 (21%)]\t\tLoss: 0.001706\n","[180/800 (22%)]\t\tLoss: 0.001544\n","[190/800 (24%)]\t\tLoss: 0.001384\n","[200/800 (25%)]\t\tLoss: 0.001278\n","[210/800 (26%)]\t\tLoss: 0.001144\n","[220/800 (28%)]\t\tLoss: 0.001043\n","[230/800 (29%)]\t\tLoss: 0.000977\n","[240/800 (30%)]\t\tLoss: 0.000945\n","[250/800 (31%)]\t\tLoss: 0.000881\n","[260/800 (32%)]\t\tLoss: 0.000828\n","[270/800 (34%)]\t\tLoss: 0.000751\n","[280/800 (35%)]\t\tLoss: 0.000722\n","[290/800 (36%)]\t\tLoss: 0.000672\n","[300/800 (38%)]\t\tLoss: 0.000658\n","[310/800 (39%)]\t\tLoss: 0.000623\n","[320/800 (40%)]\t\tLoss: 0.000576\n","[330/800 (41%)]\t\tLoss: 0.000564\n","[340/800 (42%)]\t\tLoss: 0.000629\n","[350/800 (44%)]\t\tLoss: 0.000540\n","[360/800 (45%)]\t\tLoss: 0.000519\n","[370/800 (46%)]\t\tLoss: 0.000507\n","[380/800 (48%)]\t\tLoss: 0.000499\n","[390/800 (49%)]\t\tLoss: 0.000473\n","[400/800 (50%)]\t\tLoss: 0.000462\n","[410/800 (51%)]\t\tLoss: 0.000443\n","[420/800 (52%)]\t\tLoss: 0.000441\n","[430/800 (54%)]\t\tLoss: 0.000427\n","[440/800 (55%)]\t\tLoss: 0.000414\n","[450/800 (56%)]\t\tLoss: 0.000467\n","[460/800 (58%)]\t\tLoss: 0.000414\n","[470/800 (59%)]\t\tLoss: 0.000408\n","[480/800 (60%)]\t\tLoss: 0.000439\n","[490/800 (61%)]\t\tLoss: 0.000442\n","[500/800 (62%)]\t\tLoss: 0.000382\n","[510/800 (64%)]\t\tLoss: 0.000375\n","[520/800 (65%)]\t\tLoss: 0.000421\n","[530/800 (66%)]\t\tLoss: 0.000428\n","[540/800 (68%)]\t\tLoss: 0.000365\n","[550/800 (69%)]\t\tLoss: 0.000351\n","[560/800 (70%)]\t\tLoss: 0.000360\n","[570/800 (71%)]\t\tLoss: 0.000387\n","[580/800 (72%)]\t\tLoss: 0.000338\n","[590/800 (74%)]\t\tLoss: 0.000352\n","[600/800 (75%)]\t\tLoss: 0.000361\n","[610/800 (76%)]\t\tLoss: 0.000328\n","[620/800 (78%)]\t\tLoss: 0.000327\n","[630/800 (79%)]\t\tLoss: 0.000325\n","[640/800 (80%)]\t\tLoss: 0.000402\n","[650/800 (81%)]\t\tLoss: 0.000329\n","[660/800 (82%)]\t\tLoss: 0.000393\n","[670/800 (84%)]\t\tLoss: 0.000328\n","[680/800 (85%)]\t\tLoss: 0.000336\n","[690/800 (86%)]\t\tLoss: 0.000320\n","[700/800 (88%)]\t\tLoss: 0.000311\n","[710/800 (89%)]\t\tLoss: 0.000307\n","[720/800 (90%)]\t\tLoss: 0.000308\n","[730/800 (91%)]\t\tLoss: 0.000304\n","[740/800 (92%)]\t\tLoss: 0.000290\n","[750/800 (94%)]\t\tLoss: 0.000301\n","[760/800 (95%)]\t\tLoss: 0.000288\n","[770/800 (96%)]\t\tLoss: 0.000293\n","[780/800 (98%)]\t\tLoss: 0.000285\n","[790/800 (99%)]\t\tLoss: 0.000283\n","\n","Average Training Loss: 0.002921703562969924\n","\n","Average Training Accuracy: -0.002921703562969924\n","Train Time: It tooks 393.2194s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000290\n","[10/200 (5%)]\t\tLoss: 0.000311\n","[20/200 (10%)]\t\tLoss: 0.000307\n","[30/200 (15%)]\t\tLoss: 0.000293\n","[40/200 (20%)]\t\tLoss: 0.000291\n","[50/200 (25%)]\t\tLoss: 0.000287\n","[60/200 (30%)]\t\tLoss: 0.000289\n","[70/200 (35%)]\t\tLoss: 0.000284\n","[80/200 (40%)]\t\tLoss: 0.000343\n","[90/200 (45%)]\t\tLoss: 0.000346\n","[100/200 (50%)]\t\tLoss: 0.000325\n","[110/200 (55%)]\t\tLoss: 0.000295\n","[120/200 (60%)]\t\tLoss: 0.000283\n","[130/200 (65%)]\t\tLoss: 0.000292\n","[140/200 (70%)]\t\tLoss: 0.000292\n","[150/200 (75%)]\t\tLoss: 0.000296\n","[160/200 (80%)]\t\tLoss: 0.000309\n","[170/200 (85%)]\t\tLoss: 0.000356\n","[180/200 (90%)]\t\tLoss: 0.000294\n","[190/200 (95%)]\t\tLoss: 0.000316\n","\n","Validation set: Average loss: 0.0003048903279704973\n","\n","Average Validation Accuracy: -0.0003048903279704973\n","Validation time: It tooks 94.7942s to finish the Validation.\n","\n","Starting epoch 2/10.\n","[0/800 (0%)]\t\tLoss: 0.000280\n","[10/800 (1%)]\t\tLoss: 0.000301\n","[20/800 (2%)]\t\tLoss: 0.000290\n","[30/800 (4%)]\t\tLoss: 0.000321\n","[40/800 (5%)]\t\tLoss: 0.000282\n","[50/800 (6%)]\t\tLoss: 0.000270\n","[60/800 (8%)]\t\tLoss: 0.000326\n","[70/800 (9%)]\t\tLoss: 0.000276\n","[80/800 (10%)]\t\tLoss: 0.000276\n","[90/800 (11%)]\t\tLoss: 0.000289\n","[100/800 (12%)]\t\tLoss: 0.000260\n","[110/800 (14%)]\t\tLoss: 0.000271\n","[120/800 (15%)]\t\tLoss: 0.000264\n","[130/800 (16%)]\t\tLoss: 0.000259\n","[140/800 (18%)]\t\tLoss: 0.000265\n","[150/800 (19%)]\t\tLoss: 0.000276\n","[160/800 (20%)]\t\tLoss: 0.000287\n","[170/800 (21%)]\t\tLoss: 0.000260\n","[180/800 (22%)]\t\tLoss: 0.000263\n","[190/800 (24%)]\t\tLoss: 0.000257\n","[200/800 (25%)]\t\tLoss: 0.000256\n","[210/800 (26%)]\t\tLoss: 0.000264\n","[220/800 (28%)]\t\tLoss: 0.000244\n","[230/800 (29%)]\t\tLoss: 0.000252\n","[240/800 (30%)]\t\tLoss: 0.000246\n","[250/800 (31%)]\t\tLoss: 0.000246\n","[260/800 (32%)]\t\tLoss: 0.000243\n","[270/800 (34%)]\t\tLoss: 0.000246\n","[280/800 (35%)]\t\tLoss: 0.000255\n","[290/800 (36%)]\t\tLoss: 0.000246\n","[300/800 (38%)]\t\tLoss: 0.000237\n","[310/800 (39%)]\t\tLoss: 0.000232\n","[320/800 (40%)]\t\tLoss: 0.000244\n","[330/800 (41%)]\t\tLoss: 0.000242\n","[340/800 (42%)]\t\tLoss: 0.000242\n","[350/800 (44%)]\t\tLoss: 0.000243\n","[360/800 (45%)]\t\tLoss: 0.000233\n","[370/800 (46%)]\t\tLoss: 0.000241\n","[380/800 (48%)]\t\tLoss: 0.000236\n","[390/800 (49%)]\t\tLoss: 0.000234\n","[400/800 (50%)]\t\tLoss: 0.000231\n","[410/800 (51%)]\t\tLoss: 0.000227\n","[420/800 (52%)]\t\tLoss: 0.000222\n","[430/800 (54%)]\t\tLoss: 0.000221\n","[440/800 (55%)]\t\tLoss: 0.000264\n","[450/800 (56%)]\t\tLoss: 0.000248\n","[460/800 (58%)]\t\tLoss: 0.000234\n","[470/800 (59%)]\t\tLoss: 0.000233\n","[480/800 (60%)]\t\tLoss: 0.000218\n","[490/800 (61%)]\t\tLoss: 0.000221\n","[500/800 (62%)]\t\tLoss: 0.000228\n","[510/800 (64%)]\t\tLoss: 0.000226\n","[520/800 (65%)]\t\tLoss: 0.000225\n","[530/800 (66%)]\t\tLoss: 0.000240\n","[540/800 (68%)]\t\tLoss: 0.000217\n","[550/800 (69%)]\t\tLoss: 0.000231\n","[560/800 (70%)]\t\tLoss: 0.000215\n","[570/800 (71%)]\t\tLoss: 0.000210\n","[580/800 (72%)]\t\tLoss: 0.000217\n","[590/800 (74%)]\t\tLoss: 0.000216\n","[600/800 (75%)]\t\tLoss: 0.000247\n","[610/800 (76%)]\t\tLoss: 0.000216\n","[620/800 (78%)]\t\tLoss: 0.000217\n","[630/800 (79%)]\t\tLoss: 0.000241\n","[640/800 (80%)]\t\tLoss: 0.000210\n","[650/800 (81%)]\t\tLoss: 0.000205\n","[660/800 (82%)]\t\tLoss: 0.000230\n","[670/800 (84%)]\t\tLoss: 0.000205\n","[680/800 (85%)]\t\tLoss: 0.000212\n","[690/800 (86%)]\t\tLoss: 0.000210\n","[700/800 (88%)]\t\tLoss: 0.000206\n","[710/800 (89%)]\t\tLoss: 0.000213\n","[720/800 (90%)]\t\tLoss: 0.000242\n","[730/800 (91%)]\t\tLoss: 0.000202\n","[740/800 (92%)]\t\tLoss: 0.000232\n","[750/800 (94%)]\t\tLoss: 0.000208\n","[760/800 (95%)]\t\tLoss: 0.000207\n","[770/800 (96%)]\t\tLoss: 0.000205\n","[780/800 (98%)]\t\tLoss: 0.000197\n","[790/800 (99%)]\t\tLoss: 0.000194\n","\n","Average Training Loss: 0.00024126146054186391\n","\n","Average Training Accuracy: -0.00024126146054186391\n","Train Time: It tooks 12.9832s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000197\n","[10/200 (5%)]\t\tLoss: 0.000210\n","[20/200 (10%)]\t\tLoss: 0.000206\n","[30/200 (15%)]\t\tLoss: 0.000202\n","[40/200 (20%)]\t\tLoss: 0.000201\n","[50/200 (25%)]\t\tLoss: 0.000225\n","[60/200 (30%)]\t\tLoss: 0.000217\n","[70/200 (35%)]\t\tLoss: 0.000217\n","[80/200 (40%)]\t\tLoss: 0.000210\n","[90/200 (45%)]\t\tLoss: 0.000200\n","[100/200 (50%)]\t\tLoss: 0.000213\n","[110/200 (55%)]\t\tLoss: 0.000227\n","[120/200 (60%)]\t\tLoss: 0.000209\n","[130/200 (65%)]\t\tLoss: 0.000207\n","[140/200 (70%)]\t\tLoss: 0.000208\n","[150/200 (75%)]\t\tLoss: 0.000211\n","[160/200 (80%)]\t\tLoss: 0.000201\n","[170/200 (85%)]\t\tLoss: 0.000234\n","[180/200 (90%)]\t\tLoss: 0.000227\n","[190/200 (95%)]\t\tLoss: 0.000203\n","\n","Validation set: Average loss: 0.00021125185085111298\n","\n","Average Validation Accuracy: -0.00021125185085111298\n","Validation time: It tooks 1.5043s to finish the Validation.\n","\n","Starting epoch 3/10.\n","[0/800 (0%)]\t\tLoss: 0.000196\n","[10/800 (1%)]\t\tLoss: 0.000196\n","[20/800 (2%)]\t\tLoss: 0.000220\n","[30/800 (4%)]\t\tLoss: 0.000192\n","[40/800 (5%)]\t\tLoss: 0.000191\n","[50/800 (6%)]\t\tLoss: 0.000196\n","[60/800 (8%)]\t\tLoss: 0.000217\n","[70/800 (9%)]\t\tLoss: 0.000196\n","[80/800 (10%)]\t\tLoss: 0.000200\n","[90/800 (11%)]\t\tLoss: 0.000195\n","[100/800 (12%)]\t\tLoss: 0.000201\n","[110/800 (14%)]\t\tLoss: 0.000185\n","[120/800 (15%)]\t\tLoss: 0.000193\n","[130/800 (16%)]\t\tLoss: 0.000181\n","[140/800 (18%)]\t\tLoss: 0.000188\n","[150/800 (19%)]\t\tLoss: 0.000210\n","[160/800 (20%)]\t\tLoss: 0.000175\n","[170/800 (21%)]\t\tLoss: 0.000195\n","[180/800 (22%)]\t\tLoss: 0.000199\n","[190/800 (24%)]\t\tLoss: 0.000191\n","[200/800 (25%)]\t\tLoss: 0.000180\n","[210/800 (26%)]\t\tLoss: 0.000201\n","[220/800 (28%)]\t\tLoss: 0.000174\n","[230/800 (29%)]\t\tLoss: 0.000188\n","[240/800 (30%)]\t\tLoss: 0.000180\n","[250/800 (31%)]\t\tLoss: 0.000183\n","[260/800 (32%)]\t\tLoss: 0.000206\n","[270/800 (34%)]\t\tLoss: 0.000172\n","[280/800 (35%)]\t\tLoss: 0.000193\n","[290/800 (36%)]\t\tLoss: 0.000173\n","[300/800 (38%)]\t\tLoss: 0.000181\n","[310/800 (39%)]\t\tLoss: 0.000176\n","[320/800 (40%)]\t\tLoss: 0.000181\n","[330/800 (41%)]\t\tLoss: 0.000204\n","[340/800 (42%)]\t\tLoss: 0.000200\n","[350/800 (44%)]\t\tLoss: 0.000209\n","[360/800 (45%)]\t\tLoss: 0.000178\n","[370/800 (46%)]\t\tLoss: 0.000177\n","[380/800 (48%)]\t\tLoss: 0.000195\n","[390/800 (49%)]\t\tLoss: 0.000178\n","[400/800 (50%)]\t\tLoss: 0.000181\n","[410/800 (51%)]\t\tLoss: 0.000173\n","[420/800 (52%)]\t\tLoss: 0.000169\n","[430/800 (54%)]\t\tLoss: 0.000170\n","[440/800 (55%)]\t\tLoss: 0.000171\n","[450/800 (56%)]\t\tLoss: 0.000177\n","[460/800 (58%)]\t\tLoss: 0.000181\n","[470/800 (59%)]\t\tLoss: 0.000169\n","[480/800 (60%)]\t\tLoss: 0.000166\n","[490/800 (61%)]\t\tLoss: 0.000161\n","[500/800 (62%)]\t\tLoss: 0.000183\n","[510/800 (64%)]\t\tLoss: 0.000162\n","[520/800 (65%)]\t\tLoss: 0.000167\n","[530/800 (66%)]\t\tLoss: 0.000159\n","[540/800 (68%)]\t\tLoss: 0.000161\n","[550/800 (69%)]\t\tLoss: 0.000165\n","[560/800 (70%)]\t\tLoss: 0.000163\n","[570/800 (71%)]\t\tLoss: 0.000170\n","[580/800 (72%)]\t\tLoss: 0.000164\n","[590/800 (74%)]\t\tLoss: 0.000173\n","[600/800 (75%)]\t\tLoss: 0.000155\n","[610/800 (76%)]\t\tLoss: 0.000160\n","[620/800 (78%)]\t\tLoss: 0.000160\n","[630/800 (79%)]\t\tLoss: 0.000152\n","[640/800 (80%)]\t\tLoss: 0.000161\n","[650/800 (81%)]\t\tLoss: 0.000157\n","[660/800 (82%)]\t\tLoss: 0.000151\n","[670/800 (84%)]\t\tLoss: 0.000154\n","[680/800 (85%)]\t\tLoss: 0.000160\n","[690/800 (86%)]\t\tLoss: 0.000165\n","[700/800 (88%)]\t\tLoss: 0.000168\n","[710/800 (89%)]\t\tLoss: 0.000167\n","[720/800 (90%)]\t\tLoss: 0.000155\n","[730/800 (91%)]\t\tLoss: 0.000167\n","[740/800 (92%)]\t\tLoss: 0.000150\n","[750/800 (94%)]\t\tLoss: 0.000159\n","[760/800 (95%)]\t\tLoss: 0.000152\n","[770/800 (96%)]\t\tLoss: 0.000172\n","[780/800 (98%)]\t\tLoss: 0.000160\n","[790/800 (99%)]\t\tLoss: 0.000143\n","\n","Average Training Loss: 0.00017745959830790526\n","\n","Average Training Accuracy: -0.00017745959830790526\n","Train Time: It tooks 12.8840s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000155\n","[10/200 (5%)]\t\tLoss: 0.000159\n","[20/200 (10%)]\t\tLoss: 0.000152\n","[30/200 (15%)]\t\tLoss: 0.000151\n","[40/200 (20%)]\t\tLoss: 0.000165\n","[50/200 (25%)]\t\tLoss: 0.000153\n","[60/200 (30%)]\t\tLoss: 0.000157\n","[70/200 (35%)]\t\tLoss: 0.000207\n","[80/200 (40%)]\t\tLoss: 0.000158\n","[90/200 (45%)]\t\tLoss: 0.000166\n","[100/200 (50%)]\t\tLoss: 0.000157\n","[110/200 (55%)]\t\tLoss: 0.000157\n","[120/200 (60%)]\t\tLoss: 0.000161\n","[130/200 (65%)]\t\tLoss: 0.000165\n","[140/200 (70%)]\t\tLoss: 0.000161\n","[150/200 (75%)]\t\tLoss: 0.000160\n","[160/200 (80%)]\t\tLoss: 0.000164\n","[170/200 (85%)]\t\tLoss: 0.000162\n","[180/200 (90%)]\t\tLoss: 0.000152\n","[190/200 (95%)]\t\tLoss: 0.000168\n","\n","Validation set: Average loss: 0.00016150044539244845\n","\n","Average Validation Accuracy: -0.00016150044539244845\n","Validation time: It tooks 1.4817s to finish the Validation.\n","\n","Starting epoch 4/10.\n","[0/800 (0%)]\t\tLoss: 0.000149\n","[10/800 (1%)]\t\tLoss: 0.000149\n","[20/800 (2%)]\t\tLoss: 0.000151\n","[30/800 (4%)]\t\tLoss: 0.000149\n","[40/800 (5%)]\t\tLoss: 0.000144\n","[50/800 (6%)]\t\tLoss: 0.000143\n","[60/800 (8%)]\t\tLoss: 0.000163\n","[70/800 (9%)]\t\tLoss: 0.000143\n","[80/800 (10%)]\t\tLoss: 0.000143\n","[90/800 (11%)]\t\tLoss: 0.000141\n","[100/800 (12%)]\t\tLoss: 0.000143\n","[110/800 (14%)]\t\tLoss: 0.000135\n","[120/800 (15%)]\t\tLoss: 0.000140\n","[130/800 (16%)]\t\tLoss: 0.000138\n","[140/800 (18%)]\t\tLoss: 0.000137\n","[150/800 (19%)]\t\tLoss: 0.000134\n","[160/800 (20%)]\t\tLoss: 0.000135\n","[170/800 (21%)]\t\tLoss: 0.000135\n","[180/800 (22%)]\t\tLoss: 0.000139\n","[190/800 (24%)]\t\tLoss: 0.000168\n","[200/800 (25%)]\t\tLoss: 0.000135\n","[210/800 (26%)]\t\tLoss: 0.000147\n","[220/800 (28%)]\t\tLoss: 0.000133\n","[230/800 (29%)]\t\tLoss: 0.000172\n","[240/800 (30%)]\t\tLoss: 0.000134\n","[250/800 (31%)]\t\tLoss: 0.000148\n","[260/800 (32%)]\t\tLoss: 0.000135\n","[270/800 (34%)]\t\tLoss: 0.000148\n","[280/800 (35%)]\t\tLoss: 0.000134\n","[290/800 (36%)]\t\tLoss: 0.000146\n","[300/800 (38%)]\t\tLoss: 0.000132\n","[310/800 (39%)]\t\tLoss: 0.000144\n","[320/800 (40%)]\t\tLoss: 0.000135\n","[330/800 (41%)]\t\tLoss: 0.000149\n","[340/800 (42%)]\t\tLoss: 0.000130\n","[350/800 (44%)]\t\tLoss: 0.000142\n","[360/800 (45%)]\t\tLoss: 0.000138\n","[370/800 (46%)]\t\tLoss: 0.000136\n","[380/800 (48%)]\t\tLoss: 0.000134\n","[390/800 (49%)]\t\tLoss: 0.000140\n","[400/800 (50%)]\t\tLoss: 0.000139\n","[410/800 (51%)]\t\tLoss: 0.000139\n","[420/800 (52%)]\t\tLoss: 0.000127\n","[430/800 (54%)]\t\tLoss: 0.000149\n","[440/800 (55%)]\t\tLoss: 0.000127\n","[450/800 (56%)]\t\tLoss: 0.000134\n","[460/800 (58%)]\t\tLoss: 0.000123\n","[470/800 (59%)]\t\tLoss: 0.000127\n","[480/800 (60%)]\t\tLoss: 0.000123\n","[490/800 (61%)]\t\tLoss: 0.000132\n","[500/800 (62%)]\t\tLoss: 0.000126\n","[510/800 (64%)]\t\tLoss: 0.000126\n","[520/800 (65%)]\t\tLoss: 0.000130\n","[530/800 (66%)]\t\tLoss: 0.000148\n","[540/800 (68%)]\t\tLoss: 0.000129\n","[550/800 (69%)]\t\tLoss: 0.000119\n","[560/800 (70%)]\t\tLoss: 0.000130\n","[570/800 (71%)]\t\tLoss: 0.000115\n","[580/800 (72%)]\t\tLoss: 0.000146\n","[590/800 (74%)]\t\tLoss: 0.000133\n","[600/800 (75%)]\t\tLoss: 0.000125\n","[610/800 (76%)]\t\tLoss: 0.000129\n","[620/800 (78%)]\t\tLoss: 0.000118\n","[630/800 (79%)]\t\tLoss: 0.000136\n","[640/800 (80%)]\t\tLoss: 0.000127\n","[650/800 (81%)]\t\tLoss: 0.000123\n","[660/800 (82%)]\t\tLoss: 0.000138\n","[670/800 (84%)]\t\tLoss: 0.000126\n","[680/800 (85%)]\t\tLoss: 0.000158\n","[690/800 (86%)]\t\tLoss: 0.000126\n","[700/800 (88%)]\t\tLoss: 0.000137\n","[710/800 (89%)]\t\tLoss: 0.000129\n","[720/800 (90%)]\t\tLoss: 0.000123\n","[730/800 (91%)]\t\tLoss: 0.000135\n","[740/800 (92%)]\t\tLoss: 0.000151\n","[750/800 (94%)]\t\tLoss: 0.000145\n","[760/800 (95%)]\t\tLoss: 0.000128\n","[770/800 (96%)]\t\tLoss: 0.000131\n","[780/800 (98%)]\t\tLoss: 0.000115\n","[790/800 (99%)]\t\tLoss: 0.000128\n","\n","Average Training Loss: 0.00013678249779331965\n","\n","Average Training Accuracy: -0.00013678249779331965\n","Train Time: It tooks 12.9370s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000130\n","[10/200 (5%)]\t\tLoss: 0.000166\n","[20/200 (10%)]\t\tLoss: 0.000121\n","[30/200 (15%)]\t\tLoss: 0.000121\n","[40/200 (20%)]\t\tLoss: 0.000120\n","[50/200 (25%)]\t\tLoss: 0.000125\n","[60/200 (30%)]\t\tLoss: 0.000132\n","[70/200 (35%)]\t\tLoss: 0.000119\n","[80/200 (40%)]\t\tLoss: 0.000138\n","[90/200 (45%)]\t\tLoss: 0.000120\n","[100/200 (50%)]\t\tLoss: 0.000124\n","[110/200 (55%)]\t\tLoss: 0.000138\n","[120/200 (60%)]\t\tLoss: 0.000125\n","[130/200 (65%)]\t\tLoss: 0.000132\n","[140/200 (70%)]\t\tLoss: 0.000122\n","[150/200 (75%)]\t\tLoss: 0.000118\n","[160/200 (80%)]\t\tLoss: 0.000126\n","[170/200 (85%)]\t\tLoss: 0.000128\n","[180/200 (90%)]\t\tLoss: 0.000122\n","[190/200 (95%)]\t\tLoss: 0.000119\n","\n","Validation set: Average loss: 0.0001272766174224671\n","\n","Average Validation Accuracy: -0.0001272766174224671\n","Validation time: It tooks 1.4503s to finish the Validation.\n","\n","Starting epoch 5/10.\n","[0/800 (0%)]\t\tLoss: 0.000129\n","[10/800 (1%)]\t\tLoss: 0.000108\n","[20/800 (2%)]\t\tLoss: 0.000121\n","[30/800 (4%)]\t\tLoss: 0.000122\n","[40/800 (5%)]\t\tLoss: 0.000126\n","[50/800 (6%)]\t\tLoss: 0.000114\n","[60/800 (8%)]\t\tLoss: 0.000117\n","[70/800 (9%)]\t\tLoss: 0.000114\n","[80/800 (10%)]\t\tLoss: 0.000114\n","[90/800 (11%)]\t\tLoss: 0.000116\n","[100/800 (12%)]\t\tLoss: 0.000118\n","[110/800 (14%)]\t\tLoss: 0.000127\n","[120/800 (15%)]\t\tLoss: 0.000113\n","[130/800 (16%)]\t\tLoss: 0.000116\n","[140/800 (18%)]\t\tLoss: 0.000126\n","[150/800 (19%)]\t\tLoss: 0.000115\n","[160/800 (20%)]\t\tLoss: 0.000130\n","[170/800 (21%)]\t\tLoss: 0.000120\n","[180/800 (22%)]\t\tLoss: 0.000125\n","[190/800 (24%)]\t\tLoss: 0.000118\n","[200/800 (25%)]\t\tLoss: 0.000110\n","[210/800 (26%)]\t\tLoss: 0.000126\n","[220/800 (28%)]\t\tLoss: 0.000125\n","[230/800 (29%)]\t\tLoss: 0.000108\n","[240/800 (30%)]\t\tLoss: 0.000122\n","[250/800 (31%)]\t\tLoss: 0.000116\n","[260/800 (32%)]\t\tLoss: 0.000119\n","[270/800 (34%)]\t\tLoss: 0.000114\n","[280/800 (35%)]\t\tLoss: 0.000127\n","[290/800 (36%)]\t\tLoss: 0.000125\n","[300/800 (38%)]\t\tLoss: 0.000116\n","[310/800 (39%)]\t\tLoss: 0.000121\n","[320/800 (40%)]\t\tLoss: 0.000143\n","[330/800 (41%)]\t\tLoss: 0.000112\n","[340/800 (42%)]\t\tLoss: 0.000104\n","[350/800 (44%)]\t\tLoss: 0.000113\n","[360/800 (45%)]\t\tLoss: 0.000134\n","[370/800 (46%)]\t\tLoss: 0.000104\n","[380/800 (48%)]\t\tLoss: 0.000107\n","[390/800 (49%)]\t\tLoss: 0.000111\n","[400/800 (50%)]\t\tLoss: 0.000106\n","[410/800 (51%)]\t\tLoss: 0.000118\n","[420/800 (52%)]\t\tLoss: 0.000107\n","[430/800 (54%)]\t\tLoss: 0.000108\n","[440/800 (55%)]\t\tLoss: 0.000104\n","[450/800 (56%)]\t\tLoss: 0.000100\n","[460/800 (58%)]\t\tLoss: 0.000125\n","[470/800 (59%)]\t\tLoss: 0.000104\n","[480/800 (60%)]\t\tLoss: 0.000103\n","[490/800 (61%)]\t\tLoss: 0.000103\n","[500/800 (62%)]\t\tLoss: 0.000099\n","[510/800 (64%)]\t\tLoss: 0.000122\n","[520/800 (65%)]\t\tLoss: 0.000111\n","[530/800 (66%)]\t\tLoss: 0.000102\n","[540/800 (68%)]\t\tLoss: 0.000103\n","[550/800 (69%)]\t\tLoss: 0.000120\n","[560/800 (70%)]\t\tLoss: 0.000108\n","[570/800 (71%)]\t\tLoss: 0.000112\n","[580/800 (72%)]\t\tLoss: 0.000095\n","[590/800 (74%)]\t\tLoss: 0.000108\n","[600/800 (75%)]\t\tLoss: 0.000094\n","[610/800 (76%)]\t\tLoss: 0.000099\n","[620/800 (78%)]\t\tLoss: 0.000097\n","[630/800 (79%)]\t\tLoss: 0.000109\n","[640/800 (80%)]\t\tLoss: 0.000117\n","[650/800 (81%)]\t\tLoss: 0.000100\n","[660/800 (82%)]\t\tLoss: 0.000104\n","[670/800 (84%)]\t\tLoss: 0.000096\n","[680/800 (85%)]\t\tLoss: 0.000103\n","[690/800 (86%)]\t\tLoss: 0.000105\n","[700/800 (88%)]\t\tLoss: 0.000092\n","[710/800 (89%)]\t\tLoss: 0.000102\n","[720/800 (90%)]\t\tLoss: 0.000114\n","[730/800 (91%)]\t\tLoss: 0.000097\n","[740/800 (92%)]\t\tLoss: 0.000114\n","[750/800 (94%)]\t\tLoss: 0.000106\n","[760/800 (95%)]\t\tLoss: 0.000101\n","[770/800 (96%)]\t\tLoss: 0.000101\n","[780/800 (98%)]\t\tLoss: 0.000111\n","[790/800 (99%)]\t\tLoss: 0.000104\n","\n","Average Training Loss: 0.00011216537950531347\n","\n","Average Training Accuracy: -0.00011216537950531347\n","Train Time: It tooks 12.9873s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000105\n","[10/200 (5%)]\t\tLoss: 0.000106\n","[20/200 (10%)]\t\tLoss: 0.000107\n","[30/200 (15%)]\t\tLoss: 0.000105\n","[40/200 (20%)]\t\tLoss: 0.000113\n","[50/200 (25%)]\t\tLoss: 0.000110\n","[60/200 (30%)]\t\tLoss: 0.000106\n","[70/200 (35%)]\t\tLoss: 0.000101\n","[80/200 (40%)]\t\tLoss: 0.000105\n","[90/200 (45%)]\t\tLoss: 0.000100\n","[100/200 (50%)]\t\tLoss: 0.000114\n","[110/200 (55%)]\t\tLoss: 0.000116\n","[120/200 (60%)]\t\tLoss: 0.000101\n","[130/200 (65%)]\t\tLoss: 0.000101\n","[140/200 (70%)]\t\tLoss: 0.000105\n","[150/200 (75%)]\t\tLoss: 0.000119\n","[160/200 (80%)]\t\tLoss: 0.000104\n","[170/200 (85%)]\t\tLoss: 0.000116\n","[180/200 (90%)]\t\tLoss: 0.000108\n","[190/200 (95%)]\t\tLoss: 0.000106\n","\n","Validation set: Average loss: 0.00010747346932475921\n","\n","Average Validation Accuracy: -0.00010747346932475921\n","Validation time: It tooks 1.5375s to finish the Validation.\n","\n","Starting epoch 6/10.\n","[0/800 (0%)]\t\tLoss: 0.000094\n","[10/800 (1%)]\t\tLoss: 0.000092\n","[20/800 (2%)]\t\tLoss: 0.000099\n","[30/800 (4%)]\t\tLoss: 0.000092\n","[40/800 (5%)]\t\tLoss: 0.000096\n","[50/800 (6%)]\t\tLoss: 0.000120\n","[60/800 (8%)]\t\tLoss: 0.000088\n","[70/800 (9%)]\t\tLoss: 0.000093\n","[80/800 (10%)]\t\tLoss: 0.000096\n","[90/800 (11%)]\t\tLoss: 0.000091\n","[100/800 (12%)]\t\tLoss: 0.000091\n","[110/800 (14%)]\t\tLoss: 0.000102\n","[120/800 (15%)]\t\tLoss: 0.000094\n","[130/800 (16%)]\t\tLoss: 0.000097\n","[140/800 (18%)]\t\tLoss: 0.000107\n","[150/800 (19%)]\t\tLoss: 0.000103\n","[160/800 (20%)]\t\tLoss: 0.000097\n","[170/800 (21%)]\t\tLoss: 0.000096\n","[180/800 (22%)]\t\tLoss: 0.000096\n","[190/800 (24%)]\t\tLoss: 0.000092\n","[200/800 (25%)]\t\tLoss: 0.000093\n","[210/800 (26%)]\t\tLoss: 0.000091\n","[220/800 (28%)]\t\tLoss: 0.000097\n","[230/800 (29%)]\t\tLoss: 0.000104\n","[240/800 (30%)]\t\tLoss: 0.000089\n","[250/800 (31%)]\t\tLoss: 0.000091\n","[260/800 (32%)]\t\tLoss: 0.000100\n","[270/800 (34%)]\t\tLoss: 0.000091\n","[280/800 (35%)]\t\tLoss: 0.000088\n","[290/800 (36%)]\t\tLoss: 0.000086\n","[300/800 (38%)]\t\tLoss: 0.000088\n","[310/800 (39%)]\t\tLoss: 0.000089\n","[320/800 (40%)]\t\tLoss: 0.000109\n","[330/800 (41%)]\t\tLoss: 0.000108\n","[340/800 (42%)]\t\tLoss: 0.000092\n","[350/800 (44%)]\t\tLoss: 0.000099\n","[360/800 (45%)]\t\tLoss: 0.000095\n","[370/800 (46%)]\t\tLoss: 0.000092\n","[380/800 (48%)]\t\tLoss: 0.000103\n","[390/800 (49%)]\t\tLoss: 0.000090\n","[400/800 (50%)]\t\tLoss: 0.000094\n","[410/800 (51%)]\t\tLoss: 0.000089\n","[420/800 (52%)]\t\tLoss: 0.000089\n","[430/800 (54%)]\t\tLoss: 0.000099\n","[440/800 (55%)]\t\tLoss: 0.000086\n","[450/800 (56%)]\t\tLoss: 0.000088\n","[460/800 (58%)]\t\tLoss: 0.000088\n","[470/800 (59%)]\t\tLoss: 0.000092\n","[480/800 (60%)]\t\tLoss: 0.000085\n","[490/800 (61%)]\t\tLoss: 0.000088\n","[500/800 (62%)]\t\tLoss: 0.000093\n","[510/800 (64%)]\t\tLoss: 0.000088\n","[520/800 (65%)]\t\tLoss: 0.000093\n","[530/800 (66%)]\t\tLoss: 0.000096\n","[540/800 (68%)]\t\tLoss: 0.000089\n","[550/800 (69%)]\t\tLoss: 0.000098\n","[560/800 (70%)]\t\tLoss: 0.000101\n","[570/800 (71%)]\t\tLoss: 0.000081\n","[580/800 (72%)]\t\tLoss: 0.000098\n","[590/800 (74%)]\t\tLoss: 0.000088\n","[600/800 (75%)]\t\tLoss: 0.000100\n","[610/800 (76%)]\t\tLoss: 0.000086\n","[620/800 (78%)]\t\tLoss: 0.000094\n","[630/800 (79%)]\t\tLoss: 0.000095\n","[640/800 (80%)]\t\tLoss: 0.000107\n","[650/800 (81%)]\t\tLoss: 0.000090\n","[660/800 (82%)]\t\tLoss: 0.000095\n","[670/800 (84%)]\t\tLoss: 0.000092\n","[680/800 (85%)]\t\tLoss: 0.000087\n","[690/800 (86%)]\t\tLoss: 0.000087\n","[700/800 (88%)]\t\tLoss: 0.000109\n","[710/800 (89%)]\t\tLoss: 0.000090\n","[720/800 (90%)]\t\tLoss: 0.000090\n","[730/800 (91%)]\t\tLoss: 0.000087\n","[740/800 (92%)]\t\tLoss: 0.000095\n","[750/800 (94%)]\t\tLoss: 0.000085\n","[760/800 (95%)]\t\tLoss: 0.000113\n","[770/800 (96%)]\t\tLoss: 0.000097\n","[780/800 (98%)]\t\tLoss: 0.000105\n","[790/800 (99%)]\t\tLoss: 0.000094\n","\n","Average Training Loss: 9.435971132916165e-05\n","\n","Average Training Accuracy: -9.435971132916165e-05\n","Train Time: It tooks 13.2025s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000099\n","[10/200 (5%)]\t\tLoss: 0.000094\n","[20/200 (10%)]\t\tLoss: 0.000097\n","[30/200 (15%)]\t\tLoss: 0.000103\n","[40/200 (20%)]\t\tLoss: 0.000089\n","[50/200 (25%)]\t\tLoss: 0.000103\n","[60/200 (30%)]\t\tLoss: 0.000096\n","[70/200 (35%)]\t\tLoss: 0.000092\n","[80/200 (40%)]\t\tLoss: 0.000091\n","[90/200 (45%)]\t\tLoss: 0.000091\n","[100/200 (50%)]\t\tLoss: 0.000098\n","[110/200 (55%)]\t\tLoss: 0.000098\n","[120/200 (60%)]\t\tLoss: 0.000108\n","[130/200 (65%)]\t\tLoss: 0.000096\n","[140/200 (70%)]\t\tLoss: 0.000109\n","[150/200 (75%)]\t\tLoss: 0.000109\n","[160/200 (80%)]\t\tLoss: 0.000090\n","[170/200 (85%)]\t\tLoss: 0.000119\n","[180/200 (90%)]\t\tLoss: 0.000094\n","[190/200 (95%)]\t\tLoss: 0.000091\n","\n","Validation set: Average loss: 9.836530080065131e-05\n","\n","Average Validation Accuracy: -9.836530080065131e-05\n","Validation time: It tooks 1.4642s to finish the Validation.\n","\n","Starting epoch 7/10.\n","[0/800 (0%)]\t\tLoss: 0.000091\n","[10/800 (1%)]\t\tLoss: 0.000093\n","[20/800 (2%)]\t\tLoss: 0.000087\n","[30/800 (4%)]\t\tLoss: 0.000081\n","[40/800 (5%)]\t\tLoss: 0.000106\n","[50/800 (6%)]\t\tLoss: 0.000089\n","[60/800 (8%)]\t\tLoss: 0.000084\n","[70/800 (9%)]\t\tLoss: 0.000111\n","[80/800 (10%)]\t\tLoss: 0.000087\n","[90/800 (11%)]\t\tLoss: 0.000098\n","[100/800 (12%)]\t\tLoss: 0.000094\n","[110/800 (14%)]\t\tLoss: 0.000095\n","[120/800 (15%)]\t\tLoss: 0.000133\n","[130/800 (16%)]\t\tLoss: 0.000100\n","[140/800 (18%)]\t\tLoss: 0.000085\n","[150/800 (19%)]\t\tLoss: 0.000079\n","[160/800 (20%)]\t\tLoss: 0.000080\n","[170/800 (21%)]\t\tLoss: 0.000082\n","[180/800 (22%)]\t\tLoss: 0.000089\n","[190/800 (24%)]\t\tLoss: 0.000090\n","[200/800 (25%)]\t\tLoss: 0.000076\n","[210/800 (26%)]\t\tLoss: 0.000088\n","[220/800 (28%)]\t\tLoss: 0.000081\n","[230/800 (29%)]\t\tLoss: 0.000078\n","[240/800 (30%)]\t\tLoss: 0.000090\n","[250/800 (31%)]\t\tLoss: 0.000081\n","[260/800 (32%)]\t\tLoss: 0.000083\n","[270/800 (34%)]\t\tLoss: 0.000077\n","[280/800 (35%)]\t\tLoss: 0.000077\n","[290/800 (36%)]\t\tLoss: 0.000086\n","[300/800 (38%)]\t\tLoss: 0.000079\n","[310/800 (39%)]\t\tLoss: 0.000089\n","[320/800 (40%)]\t\tLoss: 0.000081\n","[330/800 (41%)]\t\tLoss: 0.000086\n","[340/800 (42%)]\t\tLoss: 0.000076\n","[350/800 (44%)]\t\tLoss: 0.000082\n","[360/800 (45%)]\t\tLoss: 0.000079\n","[370/800 (46%)]\t\tLoss: 0.000083\n","[380/800 (48%)]\t\tLoss: 0.000079\n","[390/800 (49%)]\t\tLoss: 0.000068\n","[400/800 (50%)]\t\tLoss: 0.000077\n","[410/800 (51%)]\t\tLoss: 0.000075\n","[420/800 (52%)]\t\tLoss: 0.000080\n","[430/800 (54%)]\t\tLoss: 0.000078\n","[440/800 (55%)]\t\tLoss: 0.000094\n","[450/800 (56%)]\t\tLoss: 0.000079\n","[460/800 (58%)]\t\tLoss: 0.000085\n","[470/800 (59%)]\t\tLoss: 0.000083\n","[480/800 (60%)]\t\tLoss: 0.000082\n","[490/800 (61%)]\t\tLoss: 0.000080\n","[500/800 (62%)]\t\tLoss: 0.000072\n","[510/800 (64%)]\t\tLoss: 0.000088\n","[520/800 (65%)]\t\tLoss: 0.000078\n","[530/800 (66%)]\t\tLoss: 0.000070\n","[540/800 (68%)]\t\tLoss: 0.000077\n","[550/800 (69%)]\t\tLoss: 0.000069\n","[560/800 (70%)]\t\tLoss: 0.000083\n","[570/800 (71%)]\t\tLoss: 0.000091\n","[580/800 (72%)]\t\tLoss: 0.000081\n","[590/800 (74%)]\t\tLoss: 0.000076\n","[600/800 (75%)]\t\tLoss: 0.000071\n","[610/800 (76%)]\t\tLoss: 0.000081\n","[620/800 (78%)]\t\tLoss: 0.000100\n","[630/800 (79%)]\t\tLoss: 0.000081\n","[640/800 (80%)]\t\tLoss: 0.000090\n","[650/800 (81%)]\t\tLoss: 0.000080\n","[660/800 (82%)]\t\tLoss: 0.000073\n","[670/800 (84%)]\t\tLoss: 0.000084\n","[680/800 (85%)]\t\tLoss: 0.000075\n","[690/800 (86%)]\t\tLoss: 0.000081\n","[700/800 (88%)]\t\tLoss: 0.000077\n","[710/800 (89%)]\t\tLoss: 0.000078\n","[720/800 (90%)]\t\tLoss: 0.000079\n","[730/800 (91%)]\t\tLoss: 0.000072\n","[740/800 (92%)]\t\tLoss: 0.000084\n","[750/800 (94%)]\t\tLoss: 0.000078\n","[760/800 (95%)]\t\tLoss: 0.000072\n","[770/800 (96%)]\t\tLoss: 0.000080\n","[780/800 (98%)]\t\tLoss: 0.000074\n","[790/800 (99%)]\t\tLoss: 0.000098\n","\n","Average Training Loss: 8.353283810720313e-05\n","\n","Average Training Accuracy: -8.353283810720313e-05\n","Train Time: It tooks 12.8538s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000103\n","[10/200 (5%)]\t\tLoss: 0.000085\n","[20/200 (10%)]\t\tLoss: 0.000079\n","[30/200 (15%)]\t\tLoss: 0.000075\n","[40/200 (20%)]\t\tLoss: 0.000086\n","[50/200 (25%)]\t\tLoss: 0.000075\n","[60/200 (30%)]\t\tLoss: 0.000088\n","[70/200 (35%)]\t\tLoss: 0.000084\n","[80/200 (40%)]\t\tLoss: 0.000084\n","[90/200 (45%)]\t\tLoss: 0.000077\n","[100/200 (50%)]\t\tLoss: 0.000082\n","[110/200 (55%)]\t\tLoss: 0.000093\n","[120/200 (60%)]\t\tLoss: 0.000087\n","[130/200 (65%)]\t\tLoss: 0.000071\n","[140/200 (70%)]\t\tLoss: 0.000093\n","[150/200 (75%)]\t\tLoss: 0.000089\n","[160/200 (80%)]\t\tLoss: 0.000085\n","[170/200 (85%)]\t\tLoss: 0.000083\n","[180/200 (90%)]\t\tLoss: 0.000093\n","[190/200 (95%)]\t\tLoss: 0.000078\n","\n","Validation set: Average loss: 8.445949570159427e-05\n","\n","Average Validation Accuracy: -8.445949570159427e-05\n","Validation time: It tooks 1.4955s to finish the Validation.\n","\n","Starting epoch 8/10.\n","[0/800 (0%)]\t\tLoss: 0.000076\n","[10/800 (1%)]\t\tLoss: 0.000073\n","[20/800 (2%)]\t\tLoss: 0.000083\n","[30/800 (4%)]\t\tLoss: 0.000076\n","[40/800 (5%)]\t\tLoss: 0.000074\n","[50/800 (6%)]\t\tLoss: 0.000073\n","[60/800 (8%)]\t\tLoss: 0.000071\n","[70/800 (9%)]\t\tLoss: 0.000080\n","[80/800 (10%)]\t\tLoss: 0.000083\n","[90/800 (11%)]\t\tLoss: 0.000071\n","[100/800 (12%)]\t\tLoss: 0.000074\n","[110/800 (14%)]\t\tLoss: 0.000076\n","[120/800 (15%)]\t\tLoss: 0.000077\n","[130/800 (16%)]\t\tLoss: 0.000086\n","[140/800 (18%)]\t\tLoss: 0.000073\n","[150/800 (19%)]\t\tLoss: 0.000079\n","[160/800 (20%)]\t\tLoss: 0.000070\n","[170/800 (21%)]\t\tLoss: 0.000093\n","[180/800 (22%)]\t\tLoss: 0.000067\n","[190/800 (24%)]\t\tLoss: 0.000071\n","[200/800 (25%)]\t\tLoss: 0.000064\n","[210/800 (26%)]\t\tLoss: 0.000076\n","[220/800 (28%)]\t\tLoss: 0.000075\n","[230/800 (29%)]\t\tLoss: 0.000086\n","[240/800 (30%)]\t\tLoss: 0.000073\n","[250/800 (31%)]\t\tLoss: 0.000078\n","[260/800 (32%)]\t\tLoss: 0.000071\n","[270/800 (34%)]\t\tLoss: 0.000069\n","[280/800 (35%)]\t\tLoss: 0.000070\n","[290/800 (36%)]\t\tLoss: 0.000070\n","[300/800 (38%)]\t\tLoss: 0.000062\n","[310/800 (39%)]\t\tLoss: 0.000085\n","[320/800 (40%)]\t\tLoss: 0.000065\n","[330/800 (41%)]\t\tLoss: 0.000075\n","[340/800 (42%)]\t\tLoss: 0.000088\n","[350/800 (44%)]\t\tLoss: 0.000083\n","[360/800 (45%)]\t\tLoss: 0.000088\n","[370/800 (46%)]\t\tLoss: 0.000074\n","[380/800 (48%)]\t\tLoss: 0.000084\n","[390/800 (49%)]\t\tLoss: 0.000069\n","[400/800 (50%)]\t\tLoss: 0.000074\n","[410/800 (51%)]\t\tLoss: 0.000081\n","[420/800 (52%)]\t\tLoss: 0.000081\n","[430/800 (54%)]\t\tLoss: 0.000081\n","[440/800 (55%)]\t\tLoss: 0.000083\n","[450/800 (56%)]\t\tLoss: 0.000079\n","[460/800 (58%)]\t\tLoss: 0.000068\n","[470/800 (59%)]\t\tLoss: 0.000080\n","[480/800 (60%)]\t\tLoss: 0.000077\n","[490/800 (61%)]\t\tLoss: 0.000075\n","[500/800 (62%)]\t\tLoss: 0.000084\n","[510/800 (64%)]\t\tLoss: 0.000075\n","[520/800 (65%)]\t\tLoss: 0.000081\n","[530/800 (66%)]\t\tLoss: 0.000085\n","[540/800 (68%)]\t\tLoss: 0.000077\n","[550/800 (69%)]\t\tLoss: 0.000081\n","[560/800 (70%)]\t\tLoss: 0.000095\n","[570/800 (71%)]\t\tLoss: 0.000069\n","[580/800 (72%)]\t\tLoss: 0.000079\n","[590/800 (74%)]\t\tLoss: 0.000077\n","[600/800 (75%)]\t\tLoss: 0.000069\n","[610/800 (76%)]\t\tLoss: 0.000073\n","[620/800 (78%)]\t\tLoss: 0.000091\n","[630/800 (79%)]\t\tLoss: 0.000090\n","[640/800 (80%)]\t\tLoss: 0.000087\n","[650/800 (81%)]\t\tLoss: 0.000077\n","[660/800 (82%)]\t\tLoss: 0.000078\n","[670/800 (84%)]\t\tLoss: 0.000064\n","[680/800 (85%)]\t\tLoss: 0.000075\n","[690/800 (86%)]\t\tLoss: 0.000070\n","[700/800 (88%)]\t\tLoss: 0.000066\n","[710/800 (89%)]\t\tLoss: 0.000074\n","[720/800 (90%)]\t\tLoss: 0.000076\n","[730/800 (91%)]\t\tLoss: 0.000066\n","[740/800 (92%)]\t\tLoss: 0.000066\n","[750/800 (94%)]\t\tLoss: 0.000076\n","[760/800 (95%)]\t\tLoss: 0.000068\n","[770/800 (96%)]\t\tLoss: 0.000066\n","[780/800 (98%)]\t\tLoss: 0.000066\n","[790/800 (99%)]\t\tLoss: 0.000069\n","\n","Average Training Loss: 7.599634418511413e-05\n","\n","Average Training Accuracy: -7.599634418511413e-05\n","Train Time: It tooks 12.7954s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000077\n","[10/200 (5%)]\t\tLoss: 0.000077\n","[20/200 (10%)]\t\tLoss: 0.000068\n","[30/200 (15%)]\t\tLoss: 0.000063\n","[40/200 (20%)]\t\tLoss: 0.000066\n","[50/200 (25%)]\t\tLoss: 0.000063\n","[60/200 (30%)]\t\tLoss: 0.000077\n","[70/200 (35%)]\t\tLoss: 0.000066\n","[80/200 (40%)]\t\tLoss: 0.000071\n","[90/200 (45%)]\t\tLoss: 0.000067\n","[100/200 (50%)]\t\tLoss: 0.000080\n","[110/200 (55%)]\t\tLoss: 0.000077\n","[120/200 (60%)]\t\tLoss: 0.000071\n","[130/200 (65%)]\t\tLoss: 0.000075\n","[140/200 (70%)]\t\tLoss: 0.000080\n","[150/200 (75%)]\t\tLoss: 0.000074\n","[160/200 (80%)]\t\tLoss: 0.000091\n","[170/200 (85%)]\t\tLoss: 0.000069\n","[180/200 (90%)]\t\tLoss: 0.000068\n","[190/200 (95%)]\t\tLoss: 0.000077\n","\n","Validation set: Average loss: 7.292874688573648e-05\n","\n","Average Validation Accuracy: -7.292874688573648e-05\n","Validation time: It tooks 1.4964s to finish the Validation.\n","\n","Starting epoch 9/10.\n","[0/800 (0%)]\t\tLoss: 0.000084\n","[10/800 (1%)]\t\tLoss: 0.000075\n","[20/800 (2%)]\t\tLoss: 0.000063\n","[30/800 (4%)]\t\tLoss: 0.000075\n","[40/800 (5%)]\t\tLoss: 0.000091\n","[50/800 (6%)]\t\tLoss: 0.000074\n","[60/800 (8%)]\t\tLoss: 0.000077\n","[70/800 (9%)]\t\tLoss: 0.000076\n","[80/800 (10%)]\t\tLoss: 0.000062\n","[90/800 (11%)]\t\tLoss: 0.000074\n","[100/800 (12%)]\t\tLoss: 0.000070\n","[110/800 (14%)]\t\tLoss: 0.000090\n","[120/800 (15%)]\t\tLoss: 0.000074\n","[130/800 (16%)]\t\tLoss: 0.000069\n","[140/800 (18%)]\t\tLoss: 0.000065\n","[150/800 (19%)]\t\tLoss: 0.000066\n","[160/800 (20%)]\t\tLoss: 0.000069\n","[170/800 (21%)]\t\tLoss: 0.000065\n","[180/800 (22%)]\t\tLoss: 0.000084\n","[190/800 (24%)]\t\tLoss: 0.000067\n","[200/800 (25%)]\t\tLoss: 0.000071\n","[210/800 (26%)]\t\tLoss: 0.000076\n","[220/800 (28%)]\t\tLoss: 0.000068\n","[230/800 (29%)]\t\tLoss: 0.000075\n","[240/800 (30%)]\t\tLoss: 0.000067\n","[250/800 (31%)]\t\tLoss: 0.000068\n","[260/800 (32%)]\t\tLoss: 0.000071\n","[270/800 (34%)]\t\tLoss: 0.000068\n","[280/800 (35%)]\t\tLoss: 0.000055\n","[290/800 (36%)]\t\tLoss: 0.000062\n","[300/800 (38%)]\t\tLoss: 0.000069\n","[310/800 (39%)]\t\tLoss: 0.000069\n","[320/800 (40%)]\t\tLoss: 0.000070\n","[330/800 (41%)]\t\tLoss: 0.000064\n","[340/800 (42%)]\t\tLoss: 0.000073\n","[350/800 (44%)]\t\tLoss: 0.000065\n","[360/800 (45%)]\t\tLoss: 0.000067\n","[370/800 (46%)]\t\tLoss: 0.000070\n","[380/800 (48%)]\t\tLoss: 0.000064\n","[390/800 (49%)]\t\tLoss: 0.000057\n","[400/800 (50%)]\t\tLoss: 0.000067\n","[410/800 (51%)]\t\tLoss: 0.000073\n","[420/800 (52%)]\t\tLoss: 0.000081\n","[430/800 (54%)]\t\tLoss: 0.000073\n","[440/800 (55%)]\t\tLoss: 0.000068\n","[450/800 (56%)]\t\tLoss: 0.000074\n","[460/800 (58%)]\t\tLoss: 0.000058\n","[470/800 (59%)]\t\tLoss: 0.000074\n","[480/800 (60%)]\t\tLoss: 0.000063\n","[490/800 (61%)]\t\tLoss: 0.000079\n","[500/800 (62%)]\t\tLoss: 0.000071\n","[510/800 (64%)]\t\tLoss: 0.000071\n","[520/800 (65%)]\t\tLoss: 0.000060\n","[530/800 (66%)]\t\tLoss: 0.000067\n","[540/800 (68%)]\t\tLoss: 0.000064\n","[550/800 (69%)]\t\tLoss: 0.000071\n","[560/800 (70%)]\t\tLoss: 0.000063\n","[570/800 (71%)]\t\tLoss: 0.000077\n","[580/800 (72%)]\t\tLoss: 0.000063\n","[590/800 (74%)]\t\tLoss: 0.000073\n","[600/800 (75%)]\t\tLoss: 0.000068\n","[610/800 (76%)]\t\tLoss: 0.000068\n","[620/800 (78%)]\t\tLoss: 0.000056\n","[630/800 (79%)]\t\tLoss: 0.000059\n","[640/800 (80%)]\t\tLoss: 0.000062\n","[650/800 (81%)]\t\tLoss: 0.000056\n","[660/800 (82%)]\t\tLoss: 0.000072\n","[670/800 (84%)]\t\tLoss: 0.000061\n","[680/800 (85%)]\t\tLoss: 0.000062\n","[690/800 (86%)]\t\tLoss: 0.000056\n","[700/800 (88%)]\t\tLoss: 0.000074\n","[710/800 (89%)]\t\tLoss: 0.000057\n","[720/800 (90%)]\t\tLoss: 0.000057\n","[730/800 (91%)]\t\tLoss: 0.000071\n","[740/800 (92%)]\t\tLoss: 0.000068\n","[750/800 (94%)]\t\tLoss: 0.000057\n","[760/800 (95%)]\t\tLoss: 0.000068\n","[770/800 (96%)]\t\tLoss: 0.000057\n","[780/800 (98%)]\t\tLoss: 0.000052\n","[790/800 (99%)]\t\tLoss: 0.000065\n","\n","Average Training Loss: 6.817585240241897e-05\n","\n","Average Training Accuracy: -6.817585240241897e-05\n","Train Time: It tooks 12.8537s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000074\n","[10/200 (5%)]\t\tLoss: 0.000062\n","[20/200 (10%)]\t\tLoss: 0.000071\n","[30/200 (15%)]\t\tLoss: 0.000059\n","[40/200 (20%)]\t\tLoss: 0.000061\n","[50/200 (25%)]\t\tLoss: 0.000069\n","[60/200 (30%)]\t\tLoss: 0.000065\n","[70/200 (35%)]\t\tLoss: 0.000086\n","[80/200 (40%)]\t\tLoss: 0.000080\n","[90/200 (45%)]\t\tLoss: 0.000071\n","[100/200 (50%)]\t\tLoss: 0.000066\n","[110/200 (55%)]\t\tLoss: 0.000055\n","[120/200 (60%)]\t\tLoss: 0.000070\n","[130/200 (65%)]\t\tLoss: 0.000078\n","[140/200 (70%)]\t\tLoss: 0.000066\n","[150/200 (75%)]\t\tLoss: 0.000076\n","[160/200 (80%)]\t\tLoss: 0.000074\n","[170/200 (85%)]\t\tLoss: 0.000060\n","[180/200 (90%)]\t\tLoss: 0.000065\n","[190/200 (95%)]\t\tLoss: 0.000057\n","\n","Validation set: Average loss: 6.823509465903044e-05\n","\n","Average Validation Accuracy: -6.823509465903044e-05\n","Validation time: It tooks 1.4820s to finish the Validation.\n","\n","Starting epoch 10/10.\n","[0/800 (0%)]\t\tLoss: 0.000057\n","[10/800 (1%)]\t\tLoss: 0.000053\n","[20/800 (2%)]\t\tLoss: 0.000056\n","[30/800 (4%)]\t\tLoss: 0.000062\n","[40/800 (5%)]\t\tLoss: 0.000071\n","[50/800 (6%)]\t\tLoss: 0.000065\n","[60/800 (8%)]\t\tLoss: 0.000060\n","[70/800 (9%)]\t\tLoss: 0.000058\n","[80/800 (10%)]\t\tLoss: 0.000064\n","[90/800 (11%)]\t\tLoss: 0.000069\n","[100/800 (12%)]\t\tLoss: 0.000073\n","[110/800 (14%)]\t\tLoss: 0.000056\n","[120/800 (15%)]\t\tLoss: 0.000060\n","[130/800 (16%)]\t\tLoss: 0.000064\n","[140/800 (18%)]\t\tLoss: 0.000059\n","[150/800 (19%)]\t\tLoss: 0.000060\n","[160/800 (20%)]\t\tLoss: 0.000059\n","[170/800 (21%)]\t\tLoss: 0.000065\n","[180/800 (22%)]\t\tLoss: 0.000058\n","[190/800 (24%)]\t\tLoss: 0.000063\n","[200/800 (25%)]\t\tLoss: 0.000059\n","[210/800 (26%)]\t\tLoss: 0.000054\n","[220/800 (28%)]\t\tLoss: 0.000064\n","[230/800 (29%)]\t\tLoss: 0.000067\n","[240/800 (30%)]\t\tLoss: 0.000084\n","[250/800 (31%)]\t\tLoss: 0.000065\n","[260/800 (32%)]\t\tLoss: 0.000058\n","[270/800 (34%)]\t\tLoss: 0.000051\n","[280/800 (35%)]\t\tLoss: 0.000064\n","[290/800 (36%)]\t\tLoss: 0.000047\n","[300/800 (38%)]\t\tLoss: 0.000063\n","[310/800 (39%)]\t\tLoss: 0.000062\n","[320/800 (40%)]\t\tLoss: 0.000077\n","[330/800 (41%)]\t\tLoss: 0.000054\n","[340/800 (42%)]\t\tLoss: 0.000050\n","[350/800 (44%)]\t\tLoss: 0.000057\n","[360/800 (45%)]\t\tLoss: 0.000065\n","[370/800 (46%)]\t\tLoss: 0.000057\n","[380/800 (48%)]\t\tLoss: 0.000062\n","[390/800 (49%)]\t\tLoss: 0.000078\n","[400/800 (50%)]\t\tLoss: 0.000065\n","[410/800 (51%)]\t\tLoss: 0.000065\n","[420/800 (52%)]\t\tLoss: 0.000050\n","[430/800 (54%)]\t\tLoss: 0.000056\n","[440/800 (55%)]\t\tLoss: 0.000064\n","[450/800 (56%)]\t\tLoss: 0.000063\n","[460/800 (58%)]\t\tLoss: 0.000057\n","[470/800 (59%)]\t\tLoss: 0.000062\n","[480/800 (60%)]\t\tLoss: 0.000058\n","[490/800 (61%)]\t\tLoss: 0.000057\n","[500/800 (62%)]\t\tLoss: 0.000058\n","[510/800 (64%)]\t\tLoss: 0.000060\n","[520/800 (65%)]\t\tLoss: 0.000051\n","[530/800 (66%)]\t\tLoss: 0.000051\n","[540/800 (68%)]\t\tLoss: 0.000049\n","[550/800 (69%)]\t\tLoss: 0.000044\n","[560/800 (70%)]\t\tLoss: 0.000064\n","[570/800 (71%)]\t\tLoss: 0.000079\n","[580/800 (72%)]\t\tLoss: 0.000056\n","[590/800 (74%)]\t\tLoss: 0.000052\n","[600/800 (75%)]\t\tLoss: 0.000056\n","[610/800 (76%)]\t\tLoss: 0.000067\n","[620/800 (78%)]\t\tLoss: 0.000053\n","[630/800 (79%)]\t\tLoss: 0.000050\n","[640/800 (80%)]\t\tLoss: 0.000065\n","[650/800 (81%)]\t\tLoss: 0.000058\n","[660/800 (82%)]\t\tLoss: 0.000055\n","[670/800 (84%)]\t\tLoss: 0.000058\n","[680/800 (85%)]\t\tLoss: 0.000059\n","[690/800 (86%)]\t\tLoss: 0.000058\n","[700/800 (88%)]\t\tLoss: 0.000072\n","[710/800 (89%)]\t\tLoss: 0.000060\n","[720/800 (90%)]\t\tLoss: 0.000065\n","[730/800 (91%)]\t\tLoss: 0.000056\n","[740/800 (92%)]\t\tLoss: 0.000073\n","[750/800 (94%)]\t\tLoss: 0.000073\n","[760/800 (95%)]\t\tLoss: 0.000072\n","[770/800 (96%)]\t\tLoss: 0.000059\n","[780/800 (98%)]\t\tLoss: 0.000081\n","[790/800 (99%)]\t\tLoss: 0.000050\n","\n","Average Training Loss: 6.086789494474942e-05\n","\n","Average Training Accuracy: -6.086789494474942e-05\n","Train Time: It tooks 12.7710s to finish the epoch.\n","[0/200 (0%)]\t\tLoss: 0.000088\n","[10/200 (5%)]\t\tLoss: 0.000082\n","[20/200 (10%)]\t\tLoss: 0.000081\n","[30/200 (15%)]\t\tLoss: 0.000100\n","[40/200 (20%)]\t\tLoss: 0.000075\n","[50/200 (25%)]\t\tLoss: 0.000092\n","[60/200 (30%)]\t\tLoss: 0.000093\n","[70/200 (35%)]\t\tLoss: 0.000079\n","[80/200 (40%)]\t\tLoss: 0.000087\n","[90/200 (45%)]\t\tLoss: 0.000081\n","[100/200 (50%)]\t\tLoss: 0.000076\n","[110/200 (55%)]\t\tLoss: 0.000081\n","[120/200 (60%)]\t\tLoss: 0.000078\n","[130/200 (65%)]\t\tLoss: 0.000092\n","[140/200 (70%)]\t\tLoss: 0.000073\n","[150/200 (75%)]\t\tLoss: 0.000069\n","[160/200 (80%)]\t\tLoss: 0.000086\n","[170/200 (85%)]\t\tLoss: 0.000070\n","[180/200 (90%)]\t\tLoss: 0.000088\n","[190/200 (95%)]\t\tLoss: 0.000089\n","\n","Validation set: Average loss: 8.29555527161574e-05\n","\n","Average Validation Accuracy: -8.29555527161574e-05\n","Validation time: It tooks 1.4625s to finish the Validation.\n","It tooks 618.5609 seconds to finish the run.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(6.086789494474942e-05, 8.29555527161574e-05)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"fcAEDHR96jPT","colab_type":"text"},"source":["## Top Model"]},{"cell_type":"code","metadata":{"id":"AEPxHXwz6m3Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2ab07cfa-e7ed-41cc-8c19-bc32aaabb870","executionInfo":{"status":"error","timestamp":1566588750577,"user_tz":360,"elapsed":9565,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}}},"source":["setup_and_run_train(n_channels = 3, \n","                    n_classes = 1, \n","                    dir_img = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/sen/',\n","                    dir_gt = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/gt/', \n","                    dir_results = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/ObjectSegmentation/weights/', \n","                    load = False, \n","                    val_perc = 20, \n","                    batch_size = 10, \n","                    epochs = 25,\n","                    lr = 0.001, \n","                    run = str(1), \n","                    optimizer = \"Adam\", \n","                    loss = \"CE\", \n","                    loaders = False)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["\n","\n","    Starting training:\n","        Dataset: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/sen/\n","        Num Channels: 3\n","        Groundtruth: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/gt/\n","        Num Classes: 1\n","        Folder to save: /content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/segmentation/\n","        Load previous: False\n","        Training size: 800\n","        Validation size: 200\n","        Batch size: 10\n","        Epochs: 25\n","        Learning rate: 0.001\n","        Optimizer: Adam\n","        Loss Function: CE\n","        CUDA: True\n","    \n","\n","Starting epoch 1/25.\n","[0/800 (0%)]\t\tLoss: -0.000000\n","[10/800 (1%)]\t\tLoss: -0.000000\n","[20/800 (2%)]\t\tLoss: -0.000000\n","[30/800 (4%)]\t\tLoss: -0.000000\n","[40/800 (5%)]\t\tLoss: -0.000000\n","[50/800 (6%)]\t\tLoss: -0.000000\n","[60/800 (8%)]\t\tLoss: -0.000000\n","[70/800 (9%)]\t\tLoss: -0.000000\n","[80/800 (10%)]\t\tLoss: -0.000000\n","[90/800 (11%)]\t\tLoss: -0.000000\n","[100/800 (12%)]\t\tLoss: -0.000000\n","[110/800 (14%)]\t\tLoss: -0.000000\n","[120/800 (15%)]\t\tLoss: -0.000000\n","[130/800 (16%)]\t\tLoss: -0.000000\n","[140/800 (18%)]\t\tLoss: -0.000000\n","[150/800 (19%)]\t\tLoss: -0.000000\n","[160/800 (20%)]\t\tLoss: -0.000000\n","[170/800 (21%)]\t\tLoss: -0.000000\n","[180/800 (22%)]\t\tLoss: -0.000000\n","[190/800 (24%)]\t\tLoss: -0.000000\n","[200/800 (25%)]\t\tLoss: -0.000000\n","[210/800 (26%)]\t\tLoss: -0.000000\n","[220/800 (28%)]\t\tLoss: -0.000000\n","[230/800 (29%)]\t\tLoss: -0.000000\n","[240/800 (30%)]\t\tLoss: -0.000000\n","[250/800 (31%)]\t\tLoss: -0.000000\n","[260/800 (32%)]\t\tLoss: -0.000000\n","[270/800 (34%)]\t\tLoss: -0.000000\n","[280/800 (35%)]\t\tLoss: -0.000000\n","[290/800 (36%)]\t\tLoss: -0.000000\n","[300/800 (38%)]\t\tLoss: -0.000000\n","[310/800 (39%)]\t\tLoss: -0.000000\n","[320/800 (40%)]\t\tLoss: -0.000000\n","[330/800 (41%)]\t\tLoss: -0.000000\n","[340/800 (42%)]\t\tLoss: -0.000000\n","[350/800 (44%)]\t\tLoss: -0.000000\n","[360/800 (45%)]\t\tLoss: -0.000000\n","[370/800 (46%)]\t\tLoss: -0.000000\n","[380/800 (48%)]\t\tLoss: -0.000000\n","[390/800 (49%)]\t\tLoss: -0.000000\n","[400/800 (50%)]\t\tLoss: -0.000000\n","[410/800 (51%)]\t\tLoss: -0.000000\n","[420/800 (52%)]\t\tLoss: -0.000000\n","[430/800 (54%)]\t\tLoss: -0.000000\n","[440/800 (55%)]\t\tLoss: -0.000000\n","[450/800 (56%)]\t\tLoss: -0.000000\n","[460/800 (58%)]\t\tLoss: -0.000000\n","[470/800 (59%)]\t\tLoss: -0.000000\n","[480/800 (60%)]\t\tLoss: -0.000000\n","[490/800 (61%)]\t\tLoss: -0.000000\n","[500/800 (62%)]\t\tLoss: -0.000000\n","[510/800 (64%)]\t\tLoss: -0.000000\n","[520/800 (65%)]\t\tLoss: -0.000000\n","[530/800 (66%)]\t\tLoss: -0.000000\n","[540/800 (68%)]\t\tLoss: -0.000000\n","[550/800 (69%)]\t\tLoss: -0.000000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-3fd2ce32fea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Adam\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"CE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     loaders = False)\n\u001b[0m","\u001b[0;32m<ipython-input-14-57a3f2e45c2c>\u001b[0m in \u001b[0;36msetup_and_run_train\u001b[0;34m(n_channels, n_classes, dir_img, dir_gt, dir_results, load, val_perc, batch_size, epochs, lr, run, optimizer, loss, loaders)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nStarting epoch {}/{}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-230c6c34d235>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(net, device, loader, optimizer, criterion, batch_size, loss_type)\u001b[0m\n\u001b[1;32m     39\u001b[0m         print('[{}/{} ({:.0f}%)]\\t\\tLoss: {:.6f}'.format(\n\u001b[1;32m     40\u001b[0m             \u001b[0mbatch_idx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             100. * batch_idx / len(loader), loss.item()))\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mtime_dif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"bnT9k9h6ZXyV","colab_type":"text"},"source":["# Image from tensor"]},{"cell_type":"code","metadata":{"id":"ZiVeb3w-ZdIX","colab_type":"code","colab":{}},"source":["def to_image(tensor, nrow=8, padding=2,\n","               normalize=False, range=None, scale_each=False, pad_value=0):\n","    \"\"\"Save a given Tensor into an image file.\n","\n","    Args:\n","        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n","            saves the tensor as a grid of images by calling ``make_grid``.\n","        **kwargs: Other arguments are documented in ``make_grid``.\n","    \"\"\"\n","    from PIL import Image\n","    tensor = tensor.cpu()\n","    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n","                     normalize=normalize, range=range, scale_each=scale_each)\n","    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).numpy()\n","    im = Image.fromarray(ndarr)\n","    return im"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL6-REiCUeR7","colab_type":"text"},"source":["# Function to create a new Dataset\n","\n","This script is made to iterate over a directory applying a given set of weigths while saving the output to be made or used as a new dataset"]},{"cell_type":"code","metadata":{"id":"teY1W13-Utn_","colab_type":"code","colab":{}},"source":["def save_dataset(dir_save, load, n_classes, dir_img, conversion, out):\n","  with torch.no_grad():\n","        \n","        loader = get_dataloader_transform(dir_img)\n","        \n","        # Use GPU or not\n","        use_cuda = torch.cuda.is_available()\n","        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","        # Create the model - 256 classes are used so the resulting image can better reflect the grayscale groundtruth\n","        #net = UNet(n_channels=1, n_classes=1).to(device)\n","        # Create the model\n","        net = UNet(n_channels=1, n_classes= n_classes).to(device)\n","        net = torch.nn.DataParallel(net, device_ids=list(\n","            range(torch.cuda.device_count()))).to(device)\n","\n","        # Load old weights\n","        if load:\n","            net.load_state_dict(torch.load(load)[\"state_dict\"])\n","            print('Model loaded from {}'.format(load))\n","            \n","        net.eval()\n","        \n","        for batch_idx, (data, id_n) in enumerate(loader):\n","        \n","   \n","            # Use GPU or not\n","            data = data.to(device, dtype=torch.float)\n","            \n","            # Forward\n","            predictions = net(data)\n","            \n","            if out == \"Softmax\":\n","              predictions = F.softmax(predictions)\n","            \n","            # Saves the image\n","            img = to_image(predictions)\n","            img_new = img.convert(conversion)\n","            img_new.save(dir_save + id_n[0])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YXCwbkh0Ulno","colab_type":"text"},"source":["# Test for folder existance\n","\n","This cmd line tests if the folder exists and if colab can reach to it"]},{"cell_type":"code","metadata":{"id":"ATEHEnrGU5Nk","colab_type":"code","colab":{}},"source":["!ls predictions/DT/MAE_BDE_N/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EkeiucaDO7J","colab_type":"text"},"source":["# Create Dataset for Back Model predictions"]},{"cell_type":"markdown","metadata":{"id":"M-i5oQqeU6RV","colab_type":"text"},"source":["# Save Train Dataset"]},{"cell_type":"code","metadata":{"id":"85UcmHzCWUPR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":348},"outputId":"7fe7b872-0b79-4637-e5ee-725dd183b97f","executionInfo":{"status":"error","timestamp":1566589852940,"user_tz":360,"elapsed":390,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}}},"source":["save_dataset(dir_save='predictions/DT/K-Folds/run_4/', \n","             load='checkpoints/DT/K-Folds/weights4.pth', \n","             n_classes=1, \n","             dir_img='data/Original/',\n","             conversion='L',\n","             out = None)"],"execution_count":27,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-0cfc1fc6be96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mdir_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/Original/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mconversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m              out = None)\n\u001b[0m","\u001b[0;32m<ipython-input-26-44c791d3b5c4>\u001b[0m in \u001b[0;36msave_dataset\u001b[0;34m(dir_save, load, n_classes, dir_img, conversion, out)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Use GPU or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-9f41749013f2>\u001b[0m in \u001b[0;36mget_dataloader_transform\u001b[0;34m(dir_img, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Read the names of the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Creates the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBBBCDataset_Transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Original/'"]}]},{"cell_type":"markdown","metadata":{"id":"NWvUwPu3VGER","colab_type":"text"},"source":["# Save Test Dataset"]},{"cell_type":"code","metadata":{"id":"vYoQR7H_VJDU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":348},"outputId":"71623a63-795d-4c7b-cc3e-0d31e7f72a65","executionInfo":{"status":"error","timestamp":1566589854559,"user_tz":360,"elapsed":374,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}}},"source":["save_dataset(dir_save='predictions/DT/MAE_BDE_N/Test/', \n","             load='checkpoints/DT/weights1.pth', \n","             n_classes=1, \n","             dir_img='data/Original/Test/',\n","             conversion='L',\n","             out = None)"],"execution_count":28,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-adfe8b9f1207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mdir_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/Original/Test/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0mconversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m              out = None)\n\u001b[0m","\u001b[0;32m<ipython-input-26-44c791d3b5c4>\u001b[0m in \u001b[0;36msave_dataset\u001b[0;34m(dir_save, load, n_classes, dir_img, conversion, out)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloader_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Use GPU or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-9f41749013f2>\u001b[0m in \u001b[0;36mget_dataloader_transform\u001b[0;34m(dir_img, batch_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_dataloader_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Read the names of the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Creates the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBBBCDataset_Transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Original/Test/'"]}]},{"cell_type":"markdown","metadata":{"id":"MDJzCNBDq4o1","colab_type":"text"},"source":["# Create Dataset for Top Model predictions"]},{"cell_type":"markdown","metadata":{"id":"e0_cRMRPq80_","colab_type":"text"},"source":["# Save Train Dataset"]},{"cell_type":"code","metadata":{"id":"XQ-aRWCYq_Z4","colab_type":"code","colab":{}},"source":["save_dataset(dir_save='predictions/Border/Prueba/run_0/', \n","             load='checkpoints/Border/K-Folds/weights0.pth', \n","             n_classes=3, \n","             dir_img='data/Original/',\n","             conversion='RGB',\n","             out=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cIguL5k8rB8W","colab_type":"text"},"source":["# Save Test Dataset"]},{"cell_type":"code","metadata":{"id":"twc1-DuirJR_","colab_type":"code","colab":{}},"source":["save_dataset(dir_save='predictions/Border_Idea/Test/', \n","             load='checkpoints/Border_Idea/weights1.pth', \n","             n_classes=3, \n","             dir_img='predictions/DT/MAE_BDE_N/Test/',\n","             conversion='RGB',\n","             out=None)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKUhau-L8r7-","colab_type":"text"},"source":["# Seeing the results\n","\n","In the following charts it's the code that uses the trained model and predict some samples."]},{"cell_type":"code","metadata":{"id":"zAO60NxI8wRJ","colab_type":"code","colab":{}},"source":["def predict_imgs(net, device, loader, criterion, show=False, post=None):\n","    \n","    #net.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    \n","    i = 1\n","    \n","    with torch.no_grad():\n","        for batch_idx, (data, gt) in enumerate(loader):\n","   \n","            # Use GPU or not\n","            data, gt = data.to(device, dtype=torch.float), gt.to(device, dtype=torch.float)\n","            \n","            fig=plt.figure(figsize=(40, 40))\n","            \n","            if show:\n","              \n","                # Shows original image\n","                data_img = transforms.ToPILImage()(data.squeeze(0).cpu()).convert('RGB')\n","                fig=plt.figure(figsize=(40, 40))\n","                fig.add_subplot(1, 4, 1)\n","                plt.imshow(data_img)\n","            \n","            # Forward\n","            predictions = net(data)\n","            \n","            if post == \"Softmax\":\n","              predictions = F.softmax(predictions)\n","            \n","            #save_image(predictions, 'results/pred_' + str(i) + '.png')\n","            #save_image(gt, 'results/gt_' + str(i) + '.png')\n","            \n","            # Loss Calculation\n","            loss = criterion(predictions, gt)\n","\n","            # Updates the record\n","            val_loss.update(loss.item(), predictions.size(0))\n","            val_acc.update(-loss.item(), predictions.size(0))\n","            \n","            predictions = predictions.squeeze(0).cpu()\n","            \n","            # Shows prediction\n","            if show:\n","                # Shows original image\n","                ori = to_image(data)\n","                fig.add_subplot(1,4,1)\n","                plt.imshow(ori)\n","                # Shows prediction probability\n","                pred_p = to_image(predictions).convert('RGB')\n","                fig.add_subplot(1, 4, 2)\n","                #fig.add_subplot(1,4,1)\n","                plt.imshow(pred_p)\n","                # Shows gt\n","                gt_img = transforms.ToPILImage()(gt.squeeze(0).cpu()).convert('RGB')\n","                fig.add_subplot(1, 4, 3)\n","                #fig.add_subplot(1, 4, 2)\n","                plt.imshow(gt_img)\n","                plt.show()\n","                \n","                \n","            # Prints the loss value of every image proccessed    \n","            print('[{}/{} ({:.0f}%)]\\t'.format(\n","                batch_idx * len(data), len(loader),\n","                100. * batch_idx / len(loader)))\n","                \n","            i += 1\n","            \n","    print('\\nValidation set: Average loss: '+ str(val_loss.avg))\n","    print('\\nAverage Validation Accuracy: ' + str(val_acc.avg))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qsPaET0836N","colab_type":"code","colab":{}},"source":["def get_predloader(dir_img, dir_gt, batch_size=1):\n","    # Read the names of the images\n","    ids = [f[:-4] for f in os.listdir(dir_img)]\n","    # Rearrange the images\n","    random.shuffle(ids)\n","    # Calculate index of partition\n","    ids_pred = ids[:10]\n","\n","    # Create the datasets\n","    pred_dataset = ImageDataset(ids=ids_pred, dir_data=dir_img, dir_gt=dir_gt)\n","\n","    # Create the loaders\n","    pred_loader = DataLoader(pred_dataset, batch_size=batch_size, shuffle=True)\n","\n","    return pred_loader\n","\n","def predict(load=\"checkpoints/Border/weights1.pth\", n_channels=1, n_classes=1, dir_pred=\"data/Original/Test/\", dir_gt=\"data/Distance_Transform/Test/\", evaluation=\"MSE\", out=None):\n","\n","    # Use GPU or not\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    # Create the model - 256 classes are used so the resulting image can better reflect the grayscale groundtruth\n","    #net = UNet(n_channels=1, n_classes=1).to(device)\n","    # Create the model\n","    net = UNet(n_channels=n_channels, n_classes = n_classes).to(device)\n","    net = torch.nn.DataParallel(net, device_ids=list(\n","        range(torch.cuda.device_count()))).to(device)\n","\n","    # Load old weights\n","    if load:\n","        net.load_state_dict(torch.load(load)[\"state_dict\"])\n","        print('Model loaded from {}'.format(load))\n","    \n","        \n","    # Definition of the evaluation function\n","    if evaluation == \"MSE\":\n","        criterion_val = nn.MSELoss()\n","    elif evaluation == \"MAE\":\n","        criterion_val = nn.L1Loss()\n","        \n","    print(\"Evaluation: {}\".format(evaluation))\n","    \n","    pred_loader = get_predloader(dir_pred, dir_gt)\n","\n","    # Run the prediction\n","    predict_imgs(net=net,\n","                device=device,\n","                loader=pred_loader,\n","                criterion = criterion_val,\n","                show=True,\n","                post=out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7PJ9uFYM9G-","colab_type":"text"},"source":["# Test predictions for back model"]},{"cell_type":"code","metadata":{"id":"9jRM_rgG87Mp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Dlx7VI1dmzLwKP8VA4NVaRgoGMfitEK_"},"outputId":"f902f106-ef0e-44c6-d39f-678c9e386a53","executionInfo":{"status":"ok","timestamp":1566590106931,"user_tz":360,"elapsed":16342,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}}},"source":["predict(load= '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/ObjectSegmentation/weights/weights1.pth',\n","        n_channels = 3,\n","        n_classes = 1,\n","        dir_pred = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/sen/',\n","        dir_gt = '/content/drive/My Drive/PARMA/OpticalFlow/GitRepo/Dataset/gt/',\n","        evaluation=\"MSE\", \n","        out=None)"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"9KdNhxxlNFSV","colab_type":"text"},"source":["# Test predictions for top model"]},{"cell_type":"code","metadata":{"id":"zrxpQYYpNEW6","colab_type":"code","colab":{}},"source":["predict(load='checkpoints/Border/K-Folds/weights1.pth',\n","        n_channels = 1,\n","        n_classes = 3,\n","        dir_pred = 'data/Original/',\n","        dir_pred_2 = None,\n","        dir_gt = 'data/Border/',\n","        evaluation=\"MSE\", \n","        out=None)"],"execution_count":0,"outputs":[]}]}